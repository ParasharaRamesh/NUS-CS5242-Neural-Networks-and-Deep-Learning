{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# <u>1.0 Finding the equation for the input graph</u>\n",
    "\n",
    "### <u>Input graph</u>\n",
    "\n",
    "![graph](./resources/graph.png)\n",
    "\n",
    "### <u>Equation</u>\n",
    "\n",
    "The equation for this graph corresponds to the following formula shown below\n",
    "\n",
    "a. L = f(x)\n",
    "  -> 1 - x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [0,1)}\n",
    "  -> x - 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1,1+h)}\n",
    "  -> 1 - x + 2*h&nbsp;&nbsp;&nbsp;{x E [1+h,1+2h]}\n",
    "\n",
    "b. Therefore, the partial derivative for this function in the specific ranges mentioned above are\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}$ = $\\frac{\\partial f(x)}{\\partial x}$\n",
    "    -> -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [0,1)}\n",
    "    -> +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1,1+h)}\n",
    "    -> -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1+h,1+2h]}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>1.1 What happens on Standard Gradient Descent?</u>\n",
    "\n",
    "![1.1](./resources/1_1.png)\n",
    "\n",
    "### <u>Assuming h = 0.5 means the equations and partial derivatives change like this</u>\n",
    "\n",
    "a. L = f(x)\n",
    "  -> 1 - x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [0,1)}\n",
    "  -> x - 1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1,1.5)}\n",
    "  -> 2 - x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1.5,2]}\n",
    "\n",
    "b. Therefore, the partial derivative for this function in the specific ranges mentioned above are\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}$ = $\\frac{\\partial f(x)}{\\partial x}$\n",
    "    -> -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [0,1)}\n",
    "    -> +1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1,1.5)}\n",
    "    -> -1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{x E [1.5,2]}\n",
    "\n",
    "### <u>Derivation</u>\n",
    "\n",
    "let x0 = 0\n",
    "\n",
    ". x1 = x0 - 0.3*(-1) = 0.3 {as x0 E [0,1)}\n",
    ". x2 = x1 - 0.3*(-1) = 0.3 + 0.3 = 0.6 {as x1 E [0,1)}\n",
    ". x3 = x2 - 0.3*(-1) = 0.6 + 0.3 = 0.9 {as x2 E [0, 1)}\n",
    ". x4 = x3 - 0.3*(-1) = 0.9 + 0.3 = 1.2 {as x3 E [0, 1)}\n",
    ". x5 = x4 - 0.3*(+1) = 1.2 - 0.3 = 0.9 {as x4 E [1, 1.5)}\n",
    "\n",
    "Here we can see that x5 is the same as x3. Which means that the values will keep switching 0.9 and 1.2 forever and there will be no convergence.\n",
    "\n",
    "The natural question which arises then would be, assuming we actually stopped at the point when the curve changes equation to x-1 would that have been a natural stopping point?\n",
    "\n",
    "In that particular case the answer is still no as there is no derivative defined for sharp points. ( if it was instead a smooth point instead of a sharp one a derivative could have probably been definable )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u> 1.3 What happens on applying adam optimizer instead? </u>\n",
    "\n",
    "![1.2](./resources/1_2.png)\n",
    "\n",
    "\n",
    "Given below is an implementation of the adam optimizer code using the parameters mentioned\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "def get_gradient(x, h):\n",
    "    '''\n",
    "\n",
    "    :param x: distance on x axis\n",
    "    :param h: height of the bump\n",
    "    :return: gradient based on where x is\n",
    "    '''\n",
    "    if x >= 0 and x < 1:\n",
    "        print(\"x is in [0,1) -> gradient is -1\")\n",
    "        return -1\n",
    "    elif x >= 1 and x < 1 + h:\n",
    "        print(\"x is in [1,1+h) -> gradient is 1\")\n",
    "        return 1\n",
    "    elif x >= 1 + h and x <= 1 + (2 * h):\n",
    "        print(\"x is in [1+h,1+2h] -> gradient is -1\")\n",
    "        return -1\n",
    "    else:\n",
    "        #undefined gradient\n",
    "        return None\n",
    "\n",
    "\n",
    "def update_m(m_t, g, beta1=0.9):\n",
    "    m_next = (beta1 * m_t) + ((1 - beta1) * g)\n",
    "    print(f\"m_t:{m_t} -> m_t+1:{m_next}\")\n",
    "    return m_next\n",
    "\n",
    "\n",
    "def update_v(v_t, g, beta2=0.999):\n",
    "    v_next = (beta2 * v_t) + ((1 - beta2) * (g ** 2))\n",
    "    print(f\"v_t:{v_t} -> v_t+1:{v_next}\")\n",
    "    return v_next\n",
    "\n",
    "\n",
    "def get_m_hat_at_t(t, m_next_t, beta1=0.9):\n",
    "    m_hat = m_next_t / (1 - (beta1 ** t))\n",
    "    print(f\"m_hat @ t:{t} is {m_hat}\")\n",
    "    return m_hat\n",
    "\n",
    "\n",
    "def get_v_hat_at_t(t, v_next_t, beta2=0.999):\n",
    "    v_hat = v_next_t / (1 - (beta2 ** t))\n",
    "    print(f\"v_hat @ t:{t} is {v_hat}\")\n",
    "    return v_hat\n",
    "\n",
    "\n",
    "def update_theta(theta_t, m_next_t_hat, v_next_t_hat, alpha=0.3, epsilon=0):\n",
    "    new_theta = theta_t - ((alpha * m_next_t_hat) / (epsilon + v_next_t_hat ** 0.5))\n",
    "    print(f\"theta old {theta_t} -> theta new {new_theta}\")\n",
    "    return new_theta\n",
    "\n",
    "\n",
    "#NOTE: h is set to some arbitrarily high value as I want to see the point where x switches to the next equation\n",
    "def adam_update(t, theta_t, m_t, v_t, h=1000000, alpha=0.3, epsilon=0, beta1=0.9, beta2=0.999):\n",
    "    t += 1\n",
    "    g = get_gradient(theta_t, h)\n",
    "    m_next_t = update_m(m_t, g, beta1)\n",
    "    v_next_t = update_v(v_t, g, beta2)\n",
    "    m_next_t_hat = get_m_hat_at_t(t, m_next_t, beta1)\n",
    "    v_next_t_hat = get_v_hat_at_t(t, v_next_t, beta2)\n",
    "    next_theta = update_theta(theta_t, m_next_t_hat, v_next_t_hat, alpha, epsilon)\n",
    "    return m_next_t, v_next_t, next_theta\n",
    "\n",
    "\n",
    "def minimize(max_iterations=100, h=1000000):\n",
    "    x = 0\n",
    "    m = 0\n",
    "    v = 0\n",
    "    convergence_threshold = 1e-5\n",
    "    for t in range(max_iterations):\n",
    "        m, v, new_x = adam_update(t, x, m, v, h)\n",
    "        step = abs(new_x - x)\n",
    "        print(\"+\" * 100)\n",
    "        print(f\"x{t}: {x} -> x{t + 1}: {new_x} | Step: {step}\")\n",
    "        print(\"=\" * 100)\n",
    "        x = new_x\n",
    "        if step < convergence_threshold:\n",
    "            print(\"Reached convergence!\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T05:13:48.499501Z",
     "start_time": "2023-09-27T05:13:48.482456900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#NOTE: This cell just shows a run for a particular value of h where the optimizer will cross the hump\n",
    "\n",
    "minimize(max_iterations=8, h=0.41018425)\n",
    "\n",
    "#NOTE: Cell output is cleared\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### <u>1.3 Explanation</u>\n",
    "\n",
    "I implemented adam and kept logging what the values of x are after each \"adam update\" for different values of \"h\".\n",
    "\n",
    "Using binary search and simple trial & error I figured out 2 limits\n",
    "0.41018425 (crosses the hump) -> 0.41018450 ( where it cant cross)\n",
    "\n",
    "Therefore after rounding it down to 2 decimal places the answer is just 0.41"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# <u>1.4: Plotting different values related to Adam Optimizer</u>\n",
    "\n",
    "![1.4](./resources/1_4.png)\n",
    "\n",
    "In the cells below I write the code required for plotting the values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#functions\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def abs(x):\n",
    "    return abs(x)\n",
    "\n",
    "#plotting related functions\n",
    "def plot_graph(x, y, title, x_title, y_title, showScatter = True):\n",
    "    import matplotlib.pyplot as plt\n",
    "    maxim = max(y)\n",
    "    minim = min(y)\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the complex function\n",
    "    plt.plot(x, y)\n",
    "\n",
    "    # Set y-axis limits\n",
    "    plt.ylim(minim - 0.5 , maxim + 0.5)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel(x_title)\n",
    "    plt.ylabel(y_title)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    if showScatter:\n",
    "        # Plot only the points with a different color\n",
    "        plt.scatter(x, y, color='red', marker='o')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()\n",
    "\n",
    "#related to adam optimization(similar to old code but slightly different!)\n",
    "#TODO.x!!\n",
    "\n",
    "def compute_gradient(theta_t, fx):\n",
    "    pass\n",
    "\n",
    "\n",
    "def adam(t, theta_t, m_t, v_t, fx, alpha=0.3, epsilon=0, beta1=0.9, beta2=0.999):\n",
    "    t += 1\n",
    "    g = compute_gradient(theta_t, fx)\n",
    "    m_next_t = update_m(m_t, g, beta1)\n",
    "    v_next_t = update_v(v_t, g, beta2)\n",
    "    m_next_t_hat = get_m_hat_at_t(t, m_next_t, beta1)\n",
    "    v_next_t_hat = get_v_hat_at_t(t, v_next_t, beta2)\n",
    "    next_theta = update_theta(theta_t, m_next_t_hat, v_next_t_hat, alpha, epsilon)\n",
    "    return m_next_t, v_next_t, next_theta\n",
    "\n",
    "\n",
    "def run_adam_optimization(max_iterations=30, x = 2, fx):\n",
    "    m = 0\n",
    "    v = 0\n",
    "    convergence_threshold = 1e-5\n",
    "    for t in range(max_iterations):\n",
    "        m, v, new_x = adam_update(t, x, m, v, fx)\n",
    "        step = abs(new_x - x)\n",
    "        print(\"+\" * 100)\n",
    "        print(f\"x{t}: {x} -> x{t + 1}: {new_x} | Step: {step}\")\n",
    "        print(\"=\" * 100)\n",
    "        x = new_x\n",
    "        if step < convergence_threshold:\n",
    "            print(\"Reached convergence!\")\n",
    "            break\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T06:52:17.247970Z",
     "start_time": "2023-09-27T06:52:16.405201300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
