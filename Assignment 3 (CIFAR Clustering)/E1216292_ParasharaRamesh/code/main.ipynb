{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing python modules used later on"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import random\n",
    "!pip install pytorch_msssim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mount Google Drive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:34:33.818870800Z",
     "start_time": "2023-10-13T05:34:33.755887400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing relevant things"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:38:05.106477500Z",
     "start_time": "2023-10-13T05:38:05.048150800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from pytorch_msssim import SSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    drive_path = \"/content/drive/MyDrive\"\n",
    "    datasets_path = f\"{drive_path}/splitted_cifar10_dataset.npz\"\n",
    "    weights_path = f\"{drive_path}/weights\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sigma = 0.1\n",
    "    num_nodes = 11\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    weight_decay = 1e-4\n",
    "    grad_clip = 0.1\n",
    "\n",
    "    batch_size = 2\n",
    "    ucc_limit = 4\n",
    "    rcc_limit = 10\n",
    "    bag_size = 10\n",
    "\n",
    "config = Config()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :(40000, 32, 32, 3)\n",
      "y_train shape :(40000, 1)\n",
      "x_val shape :(10000, 32, 32, 3)\n",
      "y_val shape :(10000, 1)\n",
      "x_test shape :(10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "splitted_dataset = np.load(config.datasets_path)\n",
    "\n",
    "x_train = splitted_dataset['x_train']\n",
    "print(f\"x_train shape :{x_train.shape}\")\n",
    "\n",
    "y_train = splitted_dataset['y_train']\n",
    "print(f\"y_train shape :{y_train.shape}\")\n",
    "\n",
    "x_val = splitted_dataset['x_val']\n",
    "print(f\"x_val shape :{x_val.shape}\")\n",
    "\n",
    "y_val = splitted_dataset['y_val']\n",
    "print(f\"y_val shape :{y_val.shape}\")\n",
    "\n",
    "x_test = splitted_dataset['x_test']\n",
    "print(f\"x_test shape :{x_test.shape}\")\n",
    "\n",
    "y_test = splitted_dataset['y_test']\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:51:43.335046500Z",
     "start_time": "2023-10-13T05:51:43.037483400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Dataloader\n",
    "\n",
    "This dataloader moves data directly to the device when yielding data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Wrapper on top of dataloader to move tensors to device\n",
    "'''\n",
    "class DeviceDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            yield self._move_to_device(batch)\n",
    "\n",
    "    def _move_to_device(self, batch):\n",
    "        if isinstance(batch, torch.Tensor):\n",
    "            return batch.to(self.device)\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            return [self._move_to_device(item) for item in batch]\n",
    "        elif isinstance(batch, dict):\n",
    "            return {key: self._move_to_device(value) for key, value in batch.items()}\n",
    "        else:\n",
    "            return batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test,\n",
    "                 batch_size=config.batch_size, bag_size=config.bag_size,\n",
    "                 ucc_limit=config.ucc_limit, rcc_limit=config.rcc_limit\n",
    "                 ):\n",
    "        '''\n",
    "        Note these are numpy arrays\n",
    "\n",
    "        :param x_train:\n",
    "        :param y_train:\n",
    "        :param x_val:\n",
    "        :param y_val:\n",
    "        :param x_test:\n",
    "        :param y_test:\n",
    "        '''\n",
    "        self.num_classes = rcc_limit\n",
    "        self.bag_size = bag_size\n",
    "        self.ucc_limit = ucc_limit\n",
    "        self.rcc_limit = rcc_limit\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # transforms to apply\n",
    "        self.transforms = [\n",
    "            # normal\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            # random horizontal flips\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            # random vertical flips\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            # random rotations\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomRotation(14),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            # random rotations & flips\n",
    "            transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "        ]\n",
    "\n",
    "        # converting it all into a tensor (it's not yet one hotified)\n",
    "        self.x_train = torch.from_numpy(x_train)\n",
    "        self.y_train = torch.from_numpy(y_train)\n",
    "        self.x_test = torch.from_numpy(x_test)\n",
    "        self.y_test = torch.from_numpy(y_test)\n",
    "        self.x_val = torch.from_numpy(x_val)\n",
    "        self.y_val = torch.from_numpy(y_val)\n",
    "\n",
    "        # create subdatasets ([class_0_imgs, class_1_imgs,... class_9_imgs])\n",
    "        self.train_sub_datasets = self.create_sub_datasets(self.x_train, self.y_train)\n",
    "        self.test_sub_datasets = self.create_sub_datasets(self.x_test, self.y_test)\n",
    "        self.val_sub_datasets = self.create_sub_datasets(self.x_val, self.y_val)\n",
    "\n",
    "        # create dataloaders\n",
    "        print(\"Creating KDE dataloaders\")\n",
    "        self.kde_test_dataloaders = self.create_kde_dataloaders(self.test_sub_datasets)\n",
    "\n",
    "        print(\"Created KDE dataloaders, now creating autoencoder dataloaders\")\n",
    "        #batch size is 1 as we care about image level features anyway\n",
    "        self.autoencoder_test_dataloaders = [DeviceDataLoader(test_sub_dataset, 1) for test_sub_dataset in\n",
    "                                             self.test_sub_datasets]\n",
    "        print(\"Created autoencoder dataloaders, now creating ucc dataloaders\")\n",
    "        self.ucc_train_dataloader, self.ucc_test_dataloader, self.ucc_val_dataloader = self.get_dataloaders_for_ucc()\n",
    "        print(\"Created ucc dataloaders, now creating rcc dataloaders\")\n",
    "        self.ucc_rcc_train_dataloader, self.ucc_rcc_test_dataloader, self.ucc_rcc_val_dataloader = self.get_dataloaders_for_ucc_and_rcc()\n",
    "\n",
    "        print(\"Initilized all dataloaders\")\n",
    "\n",
    "    # create dataloaders\n",
    "    def create_kde_dataloaders(self, sub_datasets):\n",
    "        kde_datasets = []\n",
    "\n",
    "        for chosen_class, pure_sub_dataset in tqdm(enumerate(sub_datasets)):\n",
    "            total_bags_for_pure_subset = len(pure_sub_dataset) // self.bag_size\n",
    "            bag_tensors = []\n",
    "\n",
    "            pure_sub_dataset_idx = 0\n",
    "            current_bag = self.create_bag()\n",
    "\n",
    "            while pure_sub_dataset_idx < len(pure_sub_dataset):\n",
    "                # get the image from this pure sub dataset\n",
    "                img = pure_sub_dataset[pure_sub_dataset_idx][0].permute((2, 0, 1))\n",
    "                bag_idx = pure_sub_dataset_idx % 10\n",
    "                current_bag[bag_idx] = img\n",
    "\n",
    "                if bag_idx == 9:\n",
    "                    # the last value has been filled, so add it to the total bags\n",
    "                    bag_tensors.append(torch.stack(current_bag))\n",
    "\n",
    "                    # create a new bag for the next set of bags to be filled\n",
    "                    current_bag = self.create_bag()\n",
    "                pure_sub_dataset_idx += 1\n",
    "\n",
    "            kde_datasets.append(TensorDataset(torch.stack(bag_tensors)))\n",
    "\n",
    "        print(\"Finished constructing the kde_datasets from the test dataset, now creating dataloaders\")\n",
    "\n",
    "        #NOTE. the batch size here can be different if required.\n",
    "        kde_data_loaders = [DeviceDataLoader(kde_sub_dataset, self.batch_size) for kde_sub_dataset in kde_datasets]\n",
    "        return kde_data_loaders\n",
    "\n",
    "    def get_dataloaders_for_ucc(self):\n",
    "        train_dataset_with_ucc, test_dataset_with_ucc, val_dataset_with_ucc = self.construct_datasets_with_ucc()\n",
    "        return DeviceDataLoader(train_dataset_with_ucc, self.batch_size), \\\n",
    "            DeviceDataLoader(test_dataset_with_ucc, self.batch_size), \\\n",
    "            DeviceDataLoader(val_dataset_with_ucc, self.batch_size)\n",
    "\n",
    "    def get_dataloaders_for_ucc_and_rcc(self):\n",
    "        train_dataset_with_ucc_and_rcc, test_dataset_with_ucc_and_rcc, val_dataset_with_ucc_and_rcc = self.construct_datasets_with_ucc_and_rcc()\n",
    "        return DeviceDataLoader(train_dataset_with_ucc_and_rcc, self.batch_size), \\\n",
    "            DeviceDataLoader(test_dataset_with_ucc_and_rcc, self.batch_size), \\\n",
    "            DeviceDataLoader(val_dataset_with_ucc_and_rcc, self.batch_size)\n",
    "\n",
    "    # create sub datasets\n",
    "    def create_sub_datasets(self, x, y):\n",
    "        # Initialize an empty list to store the sub-datasets\n",
    "        sub_datasets = []\n",
    "\n",
    "        # Split the original dataset into 10 sub-datasets\n",
    "        for class_label in range(self.num_classes):\n",
    "            # Select indices for the current class\n",
    "            indices = torch.where(y == class_label)[0]\n",
    "\n",
    "            # Extract data for the current class\n",
    "            x_class = x[indices]\n",
    "\n",
    "            # Create a TensorDataset for the current class\n",
    "            class_dataset = TensorDataset(x_class)\n",
    "\n",
    "            # Append the current class dataset to the list\n",
    "            sub_datasets.append(class_dataset)\n",
    "        return sub_datasets\n",
    "\n",
    "    # pick random image from ith class\n",
    "    def pick_random_from_ith_sub_dataset(self, sub_datasets, i, is_eval):\n",
    "        assert 0 <= i < self.num_classes\n",
    "        sub_dataset = sub_datasets[i]\n",
    "        sub_dataset_length = len(sub_dataset)\n",
    "        random_idx = random.randint(0, sub_dataset_length - 1)\n",
    "        random_img = sub_dataset[random_idx][0]\n",
    "        random_img = random_img.permute((2, 0, 1))\n",
    "        if not is_eval:\n",
    "            random_transform = random.choice(self.transforms)\n",
    "            random_img = random_transform(random_img)\n",
    "        return random_img\n",
    "\n",
    "    # construct UCC dataset\n",
    "    def construct_datasets_with_ucc(self):\n",
    "        train_dataset_with_ucc = self.construct_dataset_with_ucc(self.train_sub_datasets, False)\n",
    "        test_dataset_with_ucc = self.construct_dataset_with_ucc(self.test_sub_datasets, True)\n",
    "        val_dataset_with_ucc = self.construct_dataset_with_ucc(self.val_sub_datasets, True)\n",
    "\n",
    "        return train_dataset_with_ucc, test_dataset_with_ucc, val_dataset_with_ucc\n",
    "\n",
    "    def construct_dataset_with_ucc(self, sub_datasets, is_eval):\n",
    "        bag_tensors = []\n",
    "        ucc_tensors = []\n",
    "\n",
    "        # calculate no of bags needed (NOTE: we are not going to pick every image here!)\n",
    "        total_bags = 0\n",
    "        for sub_dataset in sub_datasets:\n",
    "            total_bags += len(sub_dataset)\n",
    "        total_bags = total_bags // self.bag_size\n",
    "\n",
    "        # NOTE: we can technically pick more images before I am not enforcing that I am picking every image.\n",
    "        for b in tqdm(range(total_bags)):\n",
    "            # this will keep picking ucc (1 -> 4) in a cyclic manner\n",
    "            ucc = (b % self.ucc_limit) + 1\n",
    "            bag_tensor = self.create_bag()\n",
    "\n",
    "            # you are choosing random classes of size {ucc}. Using this knowledge you have to fill the bag up.\n",
    "            chosen_classes = random.sample(list(range(self.num_classes)), ucc)\n",
    "            random_bag_pos = random.sample(list(range(self.bag_size)), self.bag_size)\n",
    "\n",
    "            # fill all the values for ucc first and then fill the remaining with random sampling with replacement\n",
    "            for chosen_class, bag_pos in zip(chosen_classes, random_bag_pos[:len(chosen_classes)]):\n",
    "                bag_tensor[bag_pos] = self.pick_random_from_ith_sub_dataset(sub_datasets, chosen_class, is_eval)\n",
    "\n",
    "            # fill bag_tensor pos by pos\n",
    "            for bag_pos in random_bag_pos[len(chosen_classes):]:\n",
    "                chosen_class = random.choice(chosen_classes)\n",
    "                bag_tensor[bag_pos] = self.pick_random_from_ith_sub_dataset(sub_datasets, chosen_class, is_eval)\n",
    "\n",
    "            bag_tensors.append(torch.stack(bag_tensor))\n",
    "            ucc_tensors.append(self.one_hot(ucc, self.ucc_limit))\n",
    "\n",
    "        return TensorDataset(\n",
    "            torch.stack(bag_tensors),\n",
    "            torch.stack(ucc_tensors)\n",
    "        )\n",
    "\n",
    "    # create UCC and RCC dataset\n",
    "    def construct_datasets_with_ucc_and_rcc(self):\n",
    "        train_dataset_with_ucc_and_rcc = self.construct_dataset_with_ucc_and_rcc(self.train_sub_datasets, False)\n",
    "        test_dataset_with_ucc_and_rcc = self.construct_dataset_with_ucc_and_rcc(self.val_sub_datasets, True)\n",
    "        val_dataset_with_ucc_and_rcc = self.construct_dataset_with_ucc_and_rcc(self.test_sub_datasets, True)\n",
    "\n",
    "        return train_dataset_with_ucc_and_rcc, test_dataset_with_ucc_and_rcc, val_dataset_with_ucc_and_rcc\n",
    "\n",
    "    def construct_dataset_with_ucc_and_rcc(self, sub_datasets, is_eval):\n",
    "        bag_tensors = []\n",
    "        ucc_tensors = []\n",
    "        rcc_tensors = []\n",
    "\n",
    "        # calculate no of bags needed (NOTE: we are not going to pick every image here!)\n",
    "        total_bags = 0\n",
    "        for sub_dataset in sub_datasets:\n",
    "            total_bags += len(sub_dataset)\n",
    "        total_bags = total_bags // self.bag_size\n",
    "\n",
    "        for b in tqdm(range(total_bags)):\n",
    "            # this will keep picking ucc (1 -> 4) in a cyclic manner\n",
    "            ucc = (b % self.ucc_limit) + 1\n",
    "            bag_tensor = self.create_bag()\n",
    "            rcc_tensor = [0] * self.rcc_limit\n",
    "\n",
    "            # you are choosing random classes of size {ucc}. Using this knowledge you have to fill the bag up.\n",
    "            chosen_classes = random.sample(list(range(self.num_classes)), ucc)\n",
    "            random_bag_pos = random.sample(list(range(self.bag_size)), self.bag_size)\n",
    "\n",
    "            # fill all the values for ucc first and then fill the remaining with random sampling with replacement\n",
    "            for chosen_class, bag_pos in zip(chosen_classes, random_bag_pos[:len(chosen_classes)]):\n",
    "                bag_tensor[bag_pos] = self.pick_random_from_ith_sub_dataset(sub_datasets, chosen_class, is_eval)\n",
    "                rcc_tensor[chosen_class] += 1\n",
    "\n",
    "            # fill bag_tensor pos by pos\n",
    "            for bag_pos in random_bag_pos[len(chosen_classes):]:\n",
    "                chosen_class = random.choice(chosen_classes)\n",
    "                bag_tensor[bag_pos] = self.pick_random_from_ith_sub_dataset(sub_datasets, chosen_class, is_eval)\n",
    "                rcc_tensor[chosen_class] += 1\n",
    "\n",
    "            bag_tensors.append(torch.stack(bag_tensor))\n",
    "            ucc_tensors.append(self.one_hot(ucc, self.ucc_limit))\n",
    "            rcc_tensors.append(torch.tensor(rcc_tensor))\n",
    "\n",
    "        return TensorDataset(\n",
    "            torch.stack(bag_tensors),\n",
    "            torch.stack(ucc_tensors),\n",
    "            torch.stack(rcc_tensors),\n",
    "        )\n",
    "\n",
    "    # util\n",
    "    def one_hot(self, label, limit):\n",
    "        # Create a one-hot tensor\n",
    "        one_hot = torch.zeros(limit)\n",
    "\n",
    "        # since each label is in range of [1,10] getting it to a range of [0,9]\n",
    "        one_hot[label - 1] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def create_bag(self):\n",
    "        return [None] * 10\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the dataset object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = Dataset(x_train, y_train, x_val, y_val, x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define the class names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T05:51:43.843389800Z",
     "start_time": "2023-10-13T05:51:43.794280Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model architectures"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(24, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(48 * 16, 48 * 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),  # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        reshaped_encoded = encoded.view(-1, 48, 4, 4)\n",
    "        decoded = self.decoder(reshaped_encoded)\n",
    "        return encoded, decoded\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Density Estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KDE(nn.Module):\n",
    "    def __init__(self, device=config.device, num_nodes=config.num_nodes, sigma=config.sigma):\n",
    "        super(KDE, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.sigma = sigma\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, bag_size, num_features = data.size()  # Batch, bag, J\n",
    "\n",
    "        # Create a tensor for the sample points\n",
    "        k_sample_points = torch.linspace(0, 1, steps=self.num_nodes).repeat(batch_size, bag_size, 1).to(\n",
    "            self.device)  # B, bag, num_nodes\n",
    "\n",
    "        # Constants\n",
    "        k_alfa = 1 / np.sqrt(2 * np.pi * np.square(self.sigma))\n",
    "        k_beta = -1 / (2 * self.sigma ** 2)\n",
    "\n",
    "        out_list = []\n",
    "\n",
    "        for j in range(num_features):\n",
    "            data_j = data[:, :, j]  # shape (Batch, bag)\n",
    "            temp_data = data_j.view(-1, bag_size, 1)  # shape (Batch, bag, 1)\n",
    "            temp_data = temp_data.expand(-1, -1, self.num_nodes)  # shape ( Batch, bag, num_nodes)\n",
    "\n",
    "            k_diff = k_sample_points - temp_data  # shape ( Batch, bag, num_nodes)\n",
    "            k_diff_2 = torch.square(k_diff)  # shape ( Batch, bag, num_nodes)\n",
    "            k_result = k_alfa * torch.exp(k_beta * k_diff_2)  # shape ( Batch, bag, num_nodes)\n",
    "            k_out_unnormalized = torch.sum(k_result, dim=1)  # (B, num_nodes)\n",
    "            k_norm_coeff = k_out_unnormalized.sum(dim=1).view(batch_size, 1)  # (B,1)\n",
    "            k_out = k_out_unnormalized / k_norm_coeff.expand(-1, k_out_unnormalized.size(1))  # (B, num_nodes)\n",
    "\n",
    "            out_list.append(k_out)\n",
    "        # out_list is of shape (J, B, num_nodes)\n",
    "        concat_out = torch.cat(out_list, dim=-1)  # shape is (Batch, J*num_nodes)\n",
    "        return concat_out  # shape is (Batch, J*num_nodes) -> (1, 8448)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UCC Prediction model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UCCPredictor(nn.Module):\n",
    "    def __init__(self, device=config.device, ucc_limit=config.ucc_limit):\n",
    "        super().__init__()\n",
    "        # Input size: [Batch, Bag, 48*16]\n",
    "        # Output size: [Batch, 4]\n",
    "        self.kde = KDE(device)\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # shape 4224\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),  # shape 2112\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2112, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, ucc_limit),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        kde_prob_distributions = self.kde(x)  # shape (Batch, 8448)\n",
    "        ucc_logits = self.stack(kde_prob_distributions)  # shape (Batch, 4)\n",
    "        return ucc_logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RCC Prediction model\n",
    "\n",
    "This is the additional multi task path which predicts the \"Real Class Counts\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RCCPredictor(nn.Module):\n",
    "    def __init__(self, device=config.device, rcc_limit=config.rcc_limit):\n",
    "        super().__init__()\n",
    "        # Input size: [Batch, Bag, 48*16]\n",
    "        # Output size: [Batch, 4]\n",
    "        self.kde = KDE(device)\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  # shape 4224\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=2, stride=2),  # shape 2112\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2112, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, rcc_limit),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        kde_prob_distributions = self.kde(x)  # shape (Batch, 8448)\n",
    "        rcc_logits = self.stack(kde_prob_distributions)  # shape (Batch, 10)\n",
    "        return rcc_logits\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXPERIMENT-1 : UCC Model\n",
    "\n",
    "This model tries to replicate the paper where we have an autoencoder path and a ucc path.\n",
    "\n",
    "Similarly experiment-2 will be the improvement model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code for plotting the model stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_ucc_model_stats(\n",
    "        experiment, epochs,\n",
    "        ucc_training_losses, ae_training_losses, combined_training_losses,\n",
    "        ucc_training_accuracy,\n",
    "        ucc_validation_losses, ae_validation_losses, combined_validation_losses,\n",
    "        ucc_validation_accuracy\n",
    "    ):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "    # Plot training losses\n",
    "    axes[0, 0].plot(epochs, ucc_training_losses, marker=\"o\", color=\"red\", label=\"UCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, ae_training_losses, marker=\"o\", color=\"blue\", label=\"AE Training Loss\")\n",
    "    axes[0, 0].plot(epochs, combined_training_losses, marker=\"o\", color=\"green\", label=\"Combined Training Loss\")\n",
    "    axes[0, 0].set_title(f'{experiment}: Training Loss vs Epochs')\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Training Loss')\n",
    "    axes[0, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot training accuracy\n",
    "    axes[0, 1].plot(epochs, ucc_training_accuracy, marker=\"o\", color=\"red\", label=\"UCC Training Accuracy\")\n",
    "    axes[0, 1].set_title(f'{experiment}: Training Accuracy vs Epochs')\n",
    "    axes[0, 1].set_xlabel('Epochs')\n",
    "    axes[0, 1].set_ylabel('Training Accuracy')\n",
    "    axes[0, 1].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation losses\n",
    "    axes[1, 0].plot(epochs, ucc_validation_losses, marker=\"o\", color=\"red\", label=\"UCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, ae_validation_losses, marker=\"o\", color=\"blue\", label=\"AE Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, combined_validation_losses, marker=\"o\", color=\"green\", label=\"Combined Validation Loss\")\n",
    "    axes[1, 0].set_title(f'{experiment}: Validation Loss vs Epochs')\n",
    "    axes[1, 0].set_xlabel('Epochs')\n",
    "    axes[1, 0].set_ylabel('Validation Loss')\n",
    "    axes[1, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation accuracy 1,1\n",
    "    axes[1, 1].plot(epochs, ucc_validation_accuracy, marker=\"o\", color=\"red\", label=\"UCC Validation Accuracy\")\n",
    "    axes[1, 1].set_title(f'{experiment}: Validation Accuracy vs Epochs')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 1].legend()  # Display the legend\n",
    "\n",
    "    # Add space between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # close it properly\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UCC Trainer class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class UCCTrainer:\n",
    "    def __init__(self,\n",
    "                 name, autoencoder_model, ucc_predictor_model,\n",
    "                 dataset, save_dir, device=config.device):\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "\n",
    "        # data\n",
    "        self.train_loader = dataset.ucc_train_dataloader\n",
    "        self.test_loader = dataset.ucc_test_dataloader\n",
    "        self.val_loader = dataset.ucc_val_dataloader\n",
    "        self.kde_loaders = dataset.kde_test_dataloaders  # each dataloader here will return shape of (batch, bag, 3,32,32) of a pure dataset\n",
    "        self.autoencoder_loaders = dataset.autoencoder_test_dataloaders\n",
    "\n",
    "        # create the directory if it doesn't exist!\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.save_dir, self.name), exist_ok=True)\n",
    "\n",
    "        self.autoencoder_model = autoencoder_model\n",
    "        self.ucc_predictor_model = ucc_predictor_model\n",
    "\n",
    "        # Adam optimizer(s)\n",
    "        self.ae_optimizer = optim.Adam(self.autoencoder_model.parameters(), lr=config.learning_rate,\n",
    "                                       weight_decay=config.weight_decay)\n",
    "        self.ucc_optimizer = optim.Adam(self.ucc_predictor_model.parameters(), lr=config.learning_rate,\n",
    "                                        weight_decay=config.weight_decay)\n",
    "\n",
    "        # Loss criterion(s)\n",
    "        self.ae_loss_criterion = nn.MSELoss()\n",
    "        self.ucc_loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Transforms\n",
    "        self.tensor_to_img_transform = transforms.ToPILImage()\n",
    "\n",
    "        # Values which can change based on loaded checkpoint\n",
    "        self.start_epoch = 0\n",
    "        self.epoch_numbers = []\n",
    "        self.training_ae_losses = []\n",
    "        self.training_ucc_losses = []\n",
    "        self.training_losses = []\n",
    "        self.training_ucc_accuracies = []\n",
    "\n",
    "        self.val_ae_losses = []\n",
    "        self.val_ucc_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_ucc_accuracies = []\n",
    "\n",
    "        self.train_correct_predictions = 0\n",
    "        self.train_total_batches = 0\n",
    "\n",
    "    # main train code\n",
    "    def train(self,\n",
    "              num_epochs,\n",
    "              resume_epoch_num=None,\n",
    "              load_from_checkpoint=False,\n",
    "              epoch_saver_count=2):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # initialize the params from the saved checkpoint\n",
    "        self.init_params_from_checkpoint_hook(load_from_checkpoint, resume_epoch_num)\n",
    "\n",
    "        # set up scheduler\n",
    "        self.init_scheduler_hook(num_epochs)\n",
    "\n",
    "        # Custom progress bar for total epochs with color and displaying average epoch batch_loss\n",
    "        total_progress_bar = tqdm(\n",
    "            total=num_epochs, desc=f\"Total Epochs\", position=0,\n",
    "            bar_format=\"{desc}: {percentage}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\",\n",
    "            dynamic_ncols=True, ncols=100, colour='red'\n",
    "        )\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(self.start_epoch, self.start_epoch + num_epochs):\n",
    "            # Custom progress bar for each epoch with color\n",
    "            epoch_progress_bar = tqdm(\n",
    "                total=len(self.train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{self.start_epoch + num_epochs}\",\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                dynamic_ncols=True,\n",
    "                ncols=100,\n",
    "                colour='green'\n",
    "            )\n",
    "\n",
    "            # set all models to train mode\n",
    "            self.autoencoder_model.train()\n",
    "            self.ucc_predictor_model.train()\n",
    "\n",
    "            # set the epoch training batch_loss\n",
    "            epoch_training_loss = 0.0\n",
    "            epoch_ae_loss = 0.0\n",
    "            epoch_ucc_loss = 0.0\n",
    "\n",
    "            # iterate over each batch\n",
    "            for batch_idx, data in enumerate(self.train_loader):\n",
    "                images, one_hot_ucc_labels = data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                ae_loss, encoded, decoded = self.forward_propagate_autoencoder(images)\n",
    "                ucc_loss, batch_ucc_accuracy = self.forward_propogate_ucc(decoded, one_hot_ucc_labels, True)\n",
    "\n",
    "                # calculate combined loss\n",
    "                batch_loss = ae_loss + ucc_loss\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_value_(self.autoencoder_model.parameters(), config.grad_clip)\n",
    "                nn.utils.clip_grad_value_(self.ucc_predictor_model.parameters(), config.grad_clip)\n",
    "\n",
    "                # do optimizer step and zerograd for autoencoder model\n",
    "                self.ae_optimizer.step()\n",
    "                self.ae_optimizer.zero_grad()\n",
    "\n",
    "                # do optimizer step and zerograd for ucc model\n",
    "                self.ucc_optimizer.step()\n",
    "                self.ucc_optimizer.zero_grad()\n",
    "\n",
    "                # scheduler update (remove if it doesnt work!)\n",
    "                self.ae_scheduler.step()\n",
    "                self.ucc_scheduler.step()\n",
    "\n",
    "                # add to epoch batch_loss\n",
    "                epoch_training_loss += batch_loss.item()\n",
    "                epoch_ae_loss += ae_loss.item()\n",
    "                epoch_ucc_loss += ucc_loss.item()\n",
    "\n",
    "                # Update the epoch progress bar (overwrite in place)\n",
    "                batch_stats = {\n",
    "                    \"batch_loss\": batch_loss.item(),\n",
    "                    \"ae_loss\": ae_loss.item(),\n",
    "                    \"ucc_loss\": ucc_loss.item(),\n",
    "                    \"batch_ucc_acc\": batch_ucc_accuracy\n",
    "                }\n",
    "\n",
    "                epoch_progress_bar.set_postfix(batch_stats)\n",
    "                epoch_progress_bar.update(1)\n",
    "\n",
    "            # close the epoch progress bar\n",
    "            epoch_progress_bar.close()\n",
    "\n",
    "            # calculate average epoch train statistics\n",
    "            avg_train_stats = self.calculate_avg_train_stats_hook(epoch_training_loss, epoch_ae_loss, epoch_ucc_loss)\n",
    "\n",
    "            # calculate validation statistics\n",
    "            avg_val_stats = self.validation_hook()\n",
    "\n",
    "            # Store running history\n",
    "            self.store_running_history_hook(epoch, avg_train_stats, avg_val_stats)\n",
    "\n",
    "            # Show epoch stats\n",
    "            print(f\"# Epoch {epoch + 1}\")\n",
    "            epoch_postfix = self.calculate_and_print_epoch_stats_hook(avg_train_stats, avg_val_stats)\n",
    "\n",
    "            # Update the total progress bar\n",
    "            total_progress_bar.set_postfix(epoch_postfix)\n",
    "\n",
    "            # Close tqdm bar\n",
    "            total_progress_bar.update(1)\n",
    "\n",
    "            # Save model checkpoint periodically\n",
    "            need_to_save_model_checkpoint = (epoch + 1) % epoch_saver_count == 0\n",
    "            if need_to_save_model_checkpoint:\n",
    "                print(f\"Going to save model {self.name} @ Epoch:{epoch + 1}\")\n",
    "                self.save_model_checkpoint_hook(epoch, avg_train_stats, avg_val_stats)\n",
    "\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # Close the total progress bar\n",
    "        total_progress_bar.close()\n",
    "\n",
    "        # Return the current state\n",
    "        return self.get_current_running_history_state_hook()\n",
    "\n",
    "    # hooks\n",
    "    def init_params_from_checkpoint_hook(self, load_from_checkpoint, resume_epoch_num):\n",
    "        if load_from_checkpoint:\n",
    "            # NOTE: resume_epoch_num can be None here if we want to load from the most recently saved checkpoint!\n",
    "            checkpoint_path = self.get_model_checkpoint_path(resume_epoch_num)\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "            # load previous state of models\n",
    "            self.autoencoder_model.load_state_dict(checkpoint['ae_model_state_dict'])\n",
    "            self.ucc_predictor_model.load_state_dict(checkpoint['ucc_model_state_dict'])\n",
    "\n",
    "            # load previous state of optimizers\n",
    "            self.ae_optimizer.load_state_dict(checkpoint['ae_optimizer_state_dict'])\n",
    "            self.ucc_optimizer.load_state_dict(checkpoint['ucc_optimizer_state_dict'])\n",
    "\n",
    "            # Things we are keeping track of\n",
    "            self.start_epoch = checkpoint['epoch']\n",
    "            self.epoch_numbers = checkpoint['epoch_numbers']\n",
    "\n",
    "            self.training_losses = checkpoint['training_losses']\n",
    "            self.training_ae_losses = checkpoint['training_ae_losses']\n",
    "            self.training_ucc_losses = checkpoint['training_ucc_losses']\n",
    "            self.training_ucc_accuracies = checkpoint['training_ucc_accuracies']\n",
    "\n",
    "            self.val_losses = checkpoint['val_losses']\n",
    "            self.val_ae_losses = checkpoint['val_ae_losses']\n",
    "            self.val_ucc_losses = checkpoint['val_ucc_losses']\n",
    "            self.val_ucc_accuracies = checkpoint['val_ucc_accuracies']\n",
    "\n",
    "            print(f\"Model checkpoint for {self.name} is loaded from {checkpoint_path}!\")\n",
    "\n",
    "    def init_scheduler_hook(self, num_epochs):\n",
    "        # steps per epoch here is multiplied with bag size as we are doing it at an image level\n",
    "        self.ae_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.ae_optimizer,\n",
    "            config.learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "            # steps_per_epoch=len(self.train_loader) * config.bag_size # this is only if I decide to go image by image level loss as opposed to bag level loss\n",
    "        )\n",
    "\n",
    "        # here we are doing it at a bag level\n",
    "        self.ucc_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.ucc_optimizer,\n",
    "            config.learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "        )\n",
    "\n",
    "    def forward_propagate_autoencoder(self, images):\n",
    "        # data is of shape (batchsize=2,bag=10,channels=3,height=32,width=32)\n",
    "        # generally batch size of 16 is good for cifar10 so predicting 20 won't be so bad\n",
    "        batch_size, bag_size, num_channels, height, width = images.size()\n",
    "        batches_of_bag_images = images.view(batch_size * bag_size, num_channels, height, width)\n",
    "        encoded, decoded = self.autoencoder_model(\n",
    "            batches_of_bag_images)  # we are feeding in Batch*bag images of shape (3,32,32)\n",
    "        ae_loss = self.ae_loss_criterion(decoded, batches_of_bag_images)  # compares (Batch * Bag, 3,32,32)\n",
    "        return ae_loss, encoded, decoded\n",
    "\n",
    "    def forward_propogate_ucc(self, decoded, one_hot_ucc_labels, is_train_mode=True):\n",
    "        # decoded is of shape [Batch * Bag, 48*16] ->  make it into shape [Batch, Bag, 48*16]\n",
    "        batch_times_bag_size, feature_size = decoded.size()\n",
    "        bag_size = config.bag_size\n",
    "        batch_size = batch_times_bag_size // bag_size\n",
    "        decoded = decoded.view(batch_size, bag_size, feature_size)\n",
    "        ucc_logits = self.ucc_predictor_model(decoded)\n",
    "\n",
    "        # compute the ucc_loss\n",
    "        ucc_loss = self.ucc_loss_criterion(ucc_logits, one_hot_ucc_labels)\n",
    "\n",
    "        # compute the batch stats right here and save it\n",
    "        ucc_probs = nn.Softmax(dim=1)(ucc_logits)\n",
    "        predicted = torch.argmax(ucc_probs, 1)\n",
    "        labels = torch.argmax(one_hot_ucc_labels, 1)\n",
    "        batch_correct_predictions = (predicted == labels).sum().item()\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # calculate batchwise accuracy/ucc_loss\n",
    "        batch_ucc_accuracy = batch_correct_predictions / batch_size\n",
    "        if is_train_mode:\n",
    "            self.train_correct_predictions += batch_correct_predictions\n",
    "            self.train_total_batches += batch_size\n",
    "        else:\n",
    "            self.eval_correct_predictions += batch_correct_predictions\n",
    "            self.eval_total_batches += batch_size\n",
    "        return ucc_loss, batch_ucc_accuracy\n",
    "\n",
    "    def calculate_avg_train_stats_hook(self, epoch_training_loss, epoch_ae_loss, epoch_ucc_loss):\n",
    "        avg_training_loss_for_epoch = epoch_training_loss / len(self.train_loader)\n",
    "        avg_ae_loss_for_epoch = epoch_ae_loss / len(self.train_loader)\n",
    "        avg_ucc_loss_for_epoch = epoch_ucc_loss / len(self.train_loader)\n",
    "        avg_ucc_training_accuracy = self.train_correct_predictions / self.train_total_batches\n",
    "\n",
    "        epoch_train_stats = {\n",
    "            \"avg_training_loss\": avg_training_loss_for_epoch,\n",
    "            \"avg_ae_loss\": avg_ae_loss_for_epoch,\n",
    "            \"avg_ucc_loss\": avg_ucc_loss_for_epoch,\n",
    "            \"avg_ucc_training_accuracy\": avg_ucc_training_accuracy\n",
    "        }\n",
    "\n",
    "        # reset\n",
    "        self.train_correct_predictions = 0\n",
    "        self.train_total_batches = 0\n",
    "\n",
    "        return epoch_train_stats\n",
    "\n",
    "    def validation_hook(self):\n",
    "        # class level init\n",
    "        self.eval_correct_predictions = 0\n",
    "        self.eval_total_batches = 0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_ae_loss = 0.0\n",
    "        val_ucc_loss = 0.0\n",
    "\n",
    "        # set all models to eval mode\n",
    "        self.autoencoder_model.eval()\n",
    "        self.ucc_predictor_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx, val_data in enumerate(self.val_loader):\n",
    "                val_images, val_one_hot_ucc_labels = val_data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                val_batch_ae_loss, val_encoded, val_decoded = self.forward_propagate_autoencoder(val_images)\n",
    "                val_batch_ucc_loss, val_batch_ucc_accuracy = self.forward_propogate_ucc(val_decoded,\n",
    "                                                                                        val_one_hot_ucc_labels, False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                val_batch_loss = val_batch_ae_loss + val_batch_ucc_loss\n",
    "\n",
    "                # cummulate the losses\n",
    "                val_ae_loss += val_batch_ae_loss\n",
    "                val_ucc_loss += val_batch_ucc_loss\n",
    "                val_loss += val_batch_loss\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        avg_val_ucc_loss = val_ucc_loss / len(self.val_loader)\n",
    "        avg_val_ae_loss = val_ae_loss / len(self.val_loader)\n",
    "        avg_val_ucc_training_accuracy = self.eval_correct_predictions / self.eval_total_batches\n",
    "\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.val_loader)\n",
    "\n",
    "        return {\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"avg_val_ae_loss\": avg_val_ae_loss,\n",
    "            \"avg_val_ucc_loss\": avg_val_ucc_loss,\n",
    "            \"avg_val_ucc_training_accuracy\": avg_val_ucc_training_accuracy\n",
    "        }\n",
    "\n",
    "    def calculate_and_print_epoch_stats_hook(self, avg_train_stats, avg_val_stats):\n",
    "        epoch_loss = avg_train_stats[\"avg_training_loss\"]\n",
    "        epoch_ae_loss = avg_train_stats[\"avg_ae_loss\"]\n",
    "        epoch_ucc_loss = avg_train_stats[\"avg_ucc_loss\"]\n",
    "        epoch_ucc_accuracy = avg_train_stats[\"avg_ucc_training_accuracy\"]\n",
    "\n",
    "        epoch_val_loss = avg_val_stats[\"avg_val_loss\"]\n",
    "        epoch_val_ae_loss = avg_val_stats[\"avg_val_ae_loss\"]\n",
    "        epoch_val_ucc_loss = avg_val_stats[\"avg_val_ucc_loss\"]\n",
    "        epoch_val_ucc_accuracy = avg_val_stats[\"avg_val_ucc_training_accuracy\"]\n",
    "\n",
    "        print(\n",
    "            f\"[TRAIN]: Epoch Loss: {epoch_loss} | AE Loss: {epoch_ae_loss} | UCC Loss: {epoch_ucc_loss} | UCC Acc: {epoch_ucc_accuracy}\")\n",
    "        print(\n",
    "            f\"[VAL]: Val Loss: {epoch_val_loss} | Val AE Loss: {epoch_val_ae_loss} | Val UCC Loss: {epoch_val_ucc_loss} | Val UCC Acc: {epoch_val_ucc_accuracy}\")\n",
    "\n",
    "        return {\n",
    "            \"epoch_loss\": epoch_loss,\n",
    "            \"epoch_ae_loss\": epoch_ae_loss,\n",
    "            \"epoch_ucc_loss\": epoch_ucc_loss,\n",
    "            \"epoch_ucc_acc\": epoch_ucc_accuracy,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "            \"epoch_val_ae_loss\": epoch_val_ae_loss,\n",
    "            \"epoch_val_ucc_loss\": epoch_val_ucc_loss,\n",
    "            \"epoch_val_ucc_acc\": epoch_val_ucc_accuracy\n",
    "        }\n",
    "\n",
    "    def store_running_history_hook(self, epoch, avg_train_stats, avg_val_stats):\n",
    "        self.epoch_numbers.append(epoch + 1)\n",
    "\n",
    "        self.training_ae_losses.append(avg_train_stats[\"avg_ae_loss\"])\n",
    "        self.training_ucc_losses.append(avg_train_stats[\"avg_ucc_loss\"])\n",
    "        self.training_losses.append(avg_train_stats[\"avg_training_loss\"])\n",
    "        self.training_ucc_accuracies.append(avg_train_stats[\"avg_ucc_training_accuracy\"])\n",
    "\n",
    "        self.val_ae_losses.append(avg_val_stats[\"avg_val_ae_loss\"])\n",
    "        self.val_ucc_losses.append(avg_val_stats[\"avg_val_ucc_loss\"])\n",
    "        self.val_losses.append(avg_val_stats[\"avg_val_loss\"])\n",
    "        self.val_ucc_accuracies.append(avg_val_stats[\"avg_val_ucc_training_accuracy\"])\n",
    "\n",
    "    def get_current_running_history_state_hook(self):\n",
    "        return self.epoch_numbers, \\\n",
    "            self.training_ae_losses, self.training_ucc_losses, self.training_losses, self.training_ucc_accuracies, \\\n",
    "            self.val_ae_losses, self.val_ucc_losses, self.val_losses, self.val_ucc_accuracies\n",
    "\n",
    "    def save_model_checkpoint_hook(self, epoch):\n",
    "        # set it to train mode to save the weights (but doesn't matter apparently!)\n",
    "        self.autoencoder_model.train()\n",
    "        self.ucc_predictor_model.train()\n",
    "\n",
    "        # create the directory if it doesn't exist\n",
    "        model_save_directory = os.path.join(self.save_dir, self.name)\n",
    "        os.makedirs(model_save_directory, exist_ok=True)\n",
    "\n",
    "        # Checkpoint the model at the end of each epoch\n",
    "        checkpoint_path = os.path.join(model_save_directory, f'model_epoch_{epoch + 1}.pt')\n",
    "        torch.save(\n",
    "            {\n",
    "                'ae_model_state_dict': self.autoencoder_model.state_dict(),\n",
    "                'ucc_model_state_dict': self.ucc_predictor_model.state_dict(),\n",
    "                'ae_optimizer_state_dict': self.ae_optimizer.state_dict(),\n",
    "                'ucc_optimizer_state_dict': self.ucc_optimizer.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_numbers': self.epoch_numbers,\n",
    "                'training_losses': self.training_losses,\n",
    "                'training_ae_losses': self.training_ae_losses,\n",
    "                'training_ucc_losses': self.training_ucc_losses,\n",
    "                'training_ucc_accuracies': self.training_ucc_accuracies,\n",
    "                'val_losses': self.val_losses,\n",
    "                'val_ae_losses': self.val_ae_losses,\n",
    "                'val_ucc_losses': self.val_ucc_losses,\n",
    "                'val_ucc_accuracies': self.val_ucc_accuracies,\n",
    "            },\n",
    "            checkpoint_path\n",
    "        )\n",
    "        print(f\"Saved the model checkpoint for experiment {self.name} for epoch {epoch + 1}\")\n",
    "\n",
    "    # find the most recent file and return the path\n",
    "    def get_model_checkpoint_path(self, epoch_num=None):\n",
    "        directory = os.path.join(self.save_dir, self.name)\n",
    "        if epoch_num == None:\n",
    "            # Get a list of all files in the directory\n",
    "            files = os.listdir(directory)\n",
    "\n",
    "            # Filter out only the files (exclude directories)\n",
    "            files = [f for f in files if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "            # Sort the files by their modification time in descending order (most recent first)\n",
    "            files.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)\n",
    "\n",
    "            # Get the name of the most recently added file\n",
    "            model_file = files[0] if files else None\n",
    "        else:\n",
    "            model_file = f\"model_epoch_{epoch_num}.pt\"\n",
    "        return os.path.join(directory, model_file)\n",
    "\n",
    "    def show_sample_reconstructions(self, dataloader):\n",
    "        self.autoencoder_model.eval()\n",
    "\n",
    "        # Create a subplot grid\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(9, 9))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in dataloader:\n",
    "                val_images, _ = val_data\n",
    "\n",
    "                batch_size, bag_size, num_channels, height, width = val_images.size()\n",
    "                bag_val_images = val_images.view(batch_size * bag_size, num_channels, height, width)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                _, _, val_reconstructed_images = self.forward_propagate_autoencoder(val_images)\n",
    "\n",
    "                # take only one image from the bag\n",
    "                sample_image = bag_val_images[0]\n",
    "                predicted_image = val_reconstructed_images[0]\n",
    "\n",
    "                # get it to cpu\n",
    "                sample_image = sample_image.to(\"cpu\")\n",
    "                predicted_image = predicted_image.to(\"cpu\")\n",
    "\n",
    "                # convert to PIL Image\n",
    "                sample_image = self.tensor_to_img_transform(sample_image)\n",
    "                predicted_image = self.tensor_to_img_transform(predicted_image)\n",
    "\n",
    "                axes[0].imshow(sample_image)\n",
    "                axes[0].set_title(f\"Sample Original Image\", color='green')\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                axes[1].imshow(predicted_image)\n",
    "                axes[1].set_title(f\"Sample Reconstructed Image\", color='red')\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                # show only one image\n",
    "                break\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def test_model(self):\n",
    "        # class level init\n",
    "        self.eval_correct_predictions = 0\n",
    "        self.eval_total_batches = 0\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_ae_loss = 0.0\n",
    "        test_ucc_loss = 0.0\n",
    "\n",
    "        # set all models to eval mode\n",
    "        self.autoencoder_model.eval()\n",
    "        self.ucc_predictor_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_batch_idx, test_data in enumerate(self.test_loader):\n",
    "                test_images, test_one_hot_ucc_labels = test_data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                test_batch_ae_loss, test_encoded, test_decoded = self.forward_propagate_autoencoder(test_images)\n",
    "                test_batch_ucc_loss, test_batch_ucc_accuracy = self.forward_propogate_ucc(test_decoded,\n",
    "                                                                                          test_one_hot_ucc_labels,\n",
    "                                                                                          False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                test_batch_loss = test_batch_ae_loss + test_batch_ucc_loss\n",
    "\n",
    "                # cummulate the losses\n",
    "                test_ae_loss += test_batch_ae_loss\n",
    "                test_ucc_loss += test_batch_ucc_loss\n",
    "                test_loss += test_batch_loss\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_test_loss = test_loss / len(self.val_loader)\n",
    "        avg_test_ucc_loss = test_ucc_loss / len(self.val_loader)\n",
    "        avg_test_ae_loss = test_ae_loss / len(self.val_loader)\n",
    "        avg_test_ucc_training_accuracy = self.eval_correct_predictions / self.eval_total_batches\n",
    "\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.test_loader)\n",
    "\n",
    "        return {\n",
    "            \"avg_test_loss\": avg_test_loss,\n",
    "            \"avg_test_ae_loss\": avg_test_ae_loss,\n",
    "            \"avg_test_ucc_loss\": avg_test_ucc_loss,\n",
    "            \"avg_test_ucc_training_accuracy\": avg_test_ucc_training_accuracy\n",
    "        }\n",
    "\n",
    "    def js_divergence(self, p, q):\n",
    "        \"\"\"\n",
    "        Calculate the Jensen-Shannon Divergence between two probability distributions p and q.\n",
    "\n",
    "        Args:\n",
    "        p (torch.Tensor): Probability distribution p.\n",
    "        q (torch.Tensor): Probability distribution q.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Jensen-Shannon Divergence between p and q.\n",
    "        \"\"\"\n",
    "        # Calculate the average distribution 'm'\n",
    "        m = 0.5 * (p + q)\n",
    "\n",
    "        # Calculate the KL Divergence of 'p' and 'q' from 'm'\n",
    "        kl_div_p = F.kl_div(p.log(), m, reduction='batchmean')\n",
    "        kl_div_q = F.kl_div(q.log(), m, reduction='batchmean')\n",
    "\n",
    "        # Compute the JS Divergence\n",
    "        js_divergence = 0.5 * (kl_div_p + kl_div_q)\n",
    "\n",
    "        return js_divergence\n",
    "\n",
    "    def calculate_min_js_divergence(self):\n",
    "        num_classes = len(self.kde_loaders)\n",
    "        kde_per_class = {class_idx: 0.0 for class_idx in range(num_classes)}\n",
    "\n",
    "        # find the average kde across all classes\n",
    "        for class_idx, pure_class_kde_loader in tqdm(enumerate(self.kde_loaders)):\n",
    "            num_imgs_in_class = 0\n",
    "            for batch_idx, images in tqdm(enumerate(pure_class_kde_loader)):\n",
    "                # batch data is of shape ( Batch,bag, 3,32,32)\n",
    "                batch_size, bag_size, num_channels, height, width = images.size()\n",
    "                # reshaping to shape ( batch * bag, 3 ,32,32)\n",
    "                batches_of_bag_images = images.view(batch_size * bag_size, num_channels, height, width)\n",
    "                latent_features = self.autoencoder_model.encoder(batches_of_bag_images)  # shape (Batch * bag, 48*16)\n",
    "                batch_kde_distributions = self.ucc_predictor_model.kde(latent_features)  # shape [Batch=2, 8448]\n",
    "                num_imgs_in_class += batch_kde_distributions.size(0)\n",
    "                kde_distributions = torch.sum(batch_kde_distributions, dim=0)\n",
    "                kde_per_class[class_idx] += kde_distributions\n",
    "            kde_per_class[class_idx] /= num_imgs_in_class\n",
    "\n",
    "        # find the js_divergence\n",
    "        min_divergence = torch.inf\n",
    "        best_i = None\n",
    "        best_j = None\n",
    "        for i in range(num_classes):\n",
    "            for j in range(i + 1, num_classes):\n",
    "                divergence = self.js_divergence(kde_per_class[i], kde_per_class[j])\n",
    "                print(f\"JS Divergence between {i} & {j} is {divergence}\")\n",
    "                if divergence < min_divergence:\n",
    "                    min_divergence = divergence\n",
    "                    best_i = i\n",
    "                    best_j = j\n",
    "\n",
    "        print(f\"Min JS Divergence is {min_divergence} between classes {best_i} & {best_j}\")\n",
    "        # return the min divergence\n",
    "        return min_divergence\n",
    "\n",
    "    def calculate_clustering_accuracy(self):\n",
    "        all_latent_features = []\n",
    "        truth_labels_arr = []\n",
    "        for pure_autoencoder_loader in self.autoencoder_loaders:\n",
    "            for batch_idx, data in tqdm(enumerate(pure_autoencoder_loader)):\n",
    "                # batch data is of shape (1,3,32,32), (1,1)\n",
    "                image, label = data\n",
    "                latent_features = self.autoencoder_model.encoder(image)  # shape (1, 48*16)\n",
    "\n",
    "                latent_features = latent_features.squeeze().numpy()  # ndarray shape (48*16)\n",
    "                label = label.squeeze().numpy()  # ndarray shape (1)\n",
    "\n",
    "                all_latent_features.append(latent_features)\n",
    "                truth_labels_arr.append(label)\n",
    "\n",
    "        all_latent_features = np.array(all_latent_features)\n",
    "\n",
    "        # Do kmeans fit\n",
    "        estimator = KMeans(n_clusters=10, init='k-means++', n_init=10)\n",
    "        estimator.fit(all_latent_features)\n",
    "        predicted_clustering_labels = estimator.labels_\n",
    "\n",
    "        # Calculate accuracy\n",
    "        cost_matrix = np.zeros((10, 10))\n",
    "        num_samples = np.zeros(10)\n",
    "        for truth_val in range(10):\n",
    "            temp_sample_indices = np.where(truth_labels_arr == truth_val)[0]\n",
    "            num_samples[truth_val] = temp_sample_indices.shape[0]\n",
    "\n",
    "            temp_predicted_labels = predicted_clustering_labels[temp_sample_indices]\n",
    "\n",
    "            for predicted_val in range(10):\n",
    "                temp_matching_pairs = np.where(temp_predicted_labels == predicted_val)[0]\n",
    "                cost_matrix[truth_val, predicted_val] = 1 - (temp_matching_pairs.shape[0] / temp_sample_indices.shape[0])\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        cost = cost_matrix[row_ind, col_ind]\n",
    "\n",
    "        clustering_acc = ((1 - cost) * num_samples).sum() / num_samples.sum()\n",
    "        return clustering_acc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------#"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXPERIMENT-2 : UCC-RCC Model\n",
    "\n",
    "This model is an improvement to the original model as we are also trying to predict the RCC (Real Class Counts) as a separate multitask path. This approach in theory should improve the accuracy of the model.\n",
    "\n",
    "Additionally we use the SSIM loss for the autoencoder as that is known to be a good loss function when it comes to autoencoders.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSIM Loss definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Calculate SSIM\n",
    "        ssim_value = self.ssim(x, y)\n",
    "        # Subtract SSIM from 1\n",
    "        loss = 1 - ssim_value\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code for plotting the model stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_ucc_rcc_model_stats(\n",
    "        experiment, epochs,\n",
    "        ucc_training_losses, ae_training_losses, rcc_training_losses, ucc_rcc_training_losses, combined_training_losses,\n",
    "        ucc_training_accuracy, rcc_training_accuracy,\n",
    "        ucc_validation_losses, ae_validation_losses, rcc_validation_losses, ucc_rcc_validation_losses, combined_validation_losses,\n",
    "        ucc_validation_accuracy, rcc_validation_accuracy\n",
    "    ):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "    # Plot training losses\n",
    "    axes[0, 0].plot(epochs, ucc_training_losses, marker=\"o\", color=\"red\", label=\"UCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, ae_training_losses, marker=\"o\", color=\"blue\", label=\"AE Training Loss\")\n",
    "    axes[0, 0].plot(epochs, rcc_training_losses, marker=\"o\", color=\"yellow\", label=\"RCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, ucc_rcc_training_losses, marker=\"o\", color=\"orange\", label=\"UCC-RCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, combined_training_losses, marker=\"o\", color=\"green\", label=\"Combined Training Loss\")\n",
    "    axes[0, 0].set_title(f'{experiment}: Training Loss vs Epochs')\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Training Loss')\n",
    "    axes[0, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot training accuracy\n",
    "    axes[0, 1].plot(epochs, ucc_training_accuracy, marker=\"o\", color=\"red\", label=\"UCC Training Accuracy\")\n",
    "    axes[0, 1].plot(epochs, rcc_training_accuracy, marker=\"o\", color=\"green\", label=\"RCC Training Accuracy\")\n",
    "    axes[0, 1].set_title(f'{experiment}: Training Accuracy vs Epochs')\n",
    "    axes[0, 1].set_xlabel('Epochs')\n",
    "    axes[0, 1].set_ylabel('Training Accuracy')\n",
    "    axes[0, 1].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation losses\n",
    "    axes[1, 0].plot(epochs, ucc_validation_losses, marker=\"o\", color=\"red\", label=\"UCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, ae_validation_losses, marker=\"o\", color=\"blue\", label=\"AE Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, rcc_validation_losses, marker=\"o\", color=\"yellow\", label=\"RCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, ucc_rcc_validation_losses, marker=\"o\", color=\"orange\", label=\"UCC-RCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, combined_validation_losses, marker=\"o\", color=\"green\", label=\"Combined Validation Loss\")\n",
    "    axes[1, 0].set_title(f'{experiment}: Validation Loss vs Epochs')\n",
    "    axes[1, 0].set_xlabel('Epochs')\n",
    "    axes[1, 0].set_ylabel('Validation Loss')\n",
    "    axes[1, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation accuracy 1,1\n",
    "    axes[1, 1].plot(epochs, ucc_validation_accuracy, marker=\"o\", color=\"red\", label=\"UCC Validation Accuracy\")\n",
    "    axes[1, 1].plot(epochs, rcc_validation_accuracy, marker=\"o\", color=\"green\", label=\"RCC Validation Accuracy\")\n",
    "    axes[1, 1].set_title(f'{experiment}: Validation Accuracy vs Epochs')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 1].legend()  # Display the legend\n",
    "\n",
    "    # Add space between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # close it properly\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RCC Trainer class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RCCTrainer:\n",
    "    def __init__(self,\n",
    "                 name, autoencoder_model, ucc_predictor_model, rcc_predictor_model,\n",
    "                 dataset, save_dir, device=config.device):\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "\n",
    "        # data\n",
    "        self.train_loader = dataset.ucc_rcc_train_dataloader\n",
    "        self.test_loader = dataset.ucc_rcc_test_dataloader\n",
    "        self.val_loader = dataset.ucc_rcc_val_dataloader\n",
    "        self.kde_loaders = dataset.kde_test_dataloaders  # each dataloader here will return shape of (batch, bag, 3,32,32) of a pure dataset\n",
    "        self.autoencoder_loaders = dataset.autoencoder_test_dataloaders\n",
    "\n",
    "        # create the directory if it doesn't exist!\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.save_dir, self.name), exist_ok=True)\n",
    "\n",
    "        self.autoencoder_model = autoencoder_model\n",
    "        self.ucc_predictor_model = ucc_predictor_model\n",
    "        self.rcc_predictor_model = rcc_predictor_model\n",
    "\n",
    "        # Adam optimizer(s)\n",
    "        self.ae_optimizer = optim.Adam(self.autoencoder_model.parameters(), lr=config.learning_rate,\n",
    "                                       weight_decay=config.weight_decay)\n",
    "        self.ucc_optimizer = optim.Adam(self.ucc_predictor_model.parameters(), lr=config.learning_rate,\n",
    "                                        weight_decay=config.weight_decay)\n",
    "        self.rcc_optimizer = optim.Adam(self.rcc_predictor_model.parameters(), lr=config.learning_rate,\n",
    "                                        weight_decay=config.weight_decay)\n",
    "\n",
    "        # Loss criterion(s)\n",
    "        self.ae_loss_criterion = SSIMLoss()\n",
    "        self.ucc_loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.rcc_loss_criterion = nn.MSELoss()\n",
    "\n",
    "        # Transforms\n",
    "        self.tensor_to_img_transform = transforms.ToPILImage()\n",
    "\n",
    "        # Values which can change based on loaded checkpoint\n",
    "        self.start_epoch = 0\n",
    "        self.epoch_numbers = []\n",
    "\n",
    "        self.training_losses = []\n",
    "        self.training_ae_losses = []\n",
    "        self.training_ucc_losses = []\n",
    "        self.training_rcc_losses = []\n",
    "        self.training_ucc_accuracies = []\n",
    "        self.training_rcc_accuracies = []\n",
    "\n",
    "        self.val_losses = []\n",
    "        self.val_ae_losses = []\n",
    "        self.val_ucc_losses = []\n",
    "        self.val_rcc_losses = []\n",
    "        self.val_ucc_accuracies = []\n",
    "        self.val_rcc_accuracies = []\n",
    "\n",
    "        self.train_ucc_correct_predictions = 0\n",
    "        self.train_ucc_total_batches = 0\n",
    "\n",
    "        self.train_rcc_correct_predictions = 0\n",
    "        self.train_rcc_total_batches = 0\n",
    "\n",
    "    # main train code\n",
    "    def train(self,\n",
    "              num_epochs,\n",
    "              resume_epoch_num=None,\n",
    "              load_from_checkpoint=False,\n",
    "              epoch_saver_count=2):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # initialize the params from the saved checkpoint\n",
    "        self.init_params_from_checkpoint_hook(load_from_checkpoint, resume_epoch_num)\n",
    "\n",
    "        # set up scheduler\n",
    "        self.init_scheduler_hook(num_epochs)\n",
    "\n",
    "        # Custom progress bar for total epochs with color and displaying average epoch batch_loss\n",
    "        total_progress_bar = tqdm(\n",
    "            total=num_epochs, desc=f\"Total Epochs\", position=0,\n",
    "            bar_format=\"{desc}: {percentage}% |{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\",\n",
    "            dynamic_ncols=True, ncols=100, colour='red'\n",
    "        )\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(self.start_epoch, self.start_epoch + num_epochs):\n",
    "            # Custom progress bar for each epoch with color\n",
    "            epoch_progress_bar = tqdm(\n",
    "                total=len(self.train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{self.start_epoch + num_epochs}\",\n",
    "                position=1,\n",
    "                leave=False,\n",
    "                dynamic_ncols=True,\n",
    "                ncols=100,\n",
    "                colour='green'\n",
    "            )\n",
    "\n",
    "            # set all models to train mode\n",
    "            self.autoencoder_model.train()\n",
    "            self.ucc_predictor_model.train()\n",
    "            self.rcc_predictor_model.train()\n",
    "\n",
    "            # set the epoch training batch_loss\n",
    "            epoch_training_loss = 0.0\n",
    "            epoch_ae_loss = 0.0\n",
    "            epoch_ucc_loss = 0.0\n",
    "            epoch_rcc_loss = 0.0\n",
    "\n",
    "            # iterate over each batch\n",
    "            for batch_idx, data in enumerate(self.train_loader):\n",
    "                images, one_hot_ucc_labels, rcc_labels = data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                ae_loss, encoded, decoded = self.forward_propagate_autoencoder(images)\n",
    "                ucc_loss, batch_ucc_accuracy = self.forward_propogate_ucc(decoded, one_hot_ucc_labels, True)\n",
    "                rcc_loss, batch_rcc_accuracy = self.forward_propogate_rcc(decoded, rcc_labels, True)\n",
    "\n",
    "                # calculate combined loss\n",
    "                batch_loss = ae_loss + ucc_loss + rcc_loss\n",
    "\n",
    "                # Gradient clipping\n",
    "                nn.utils.clip_grad_value_(self.autoencoder_model.parameters(), config.grad_clip)\n",
    "                nn.utils.clip_grad_value_(self.ucc_predictor_model.parameters(), config.grad_clip)\n",
    "\n",
    "                # do optimizer step and zerograd for autoencoder model\n",
    "                self.ae_optimizer.step()\n",
    "                self.ae_optimizer.zero_grad()\n",
    "\n",
    "                # do optimizer step and zerograd for ucc model\n",
    "                self.ucc_optimizer.step()\n",
    "                self.ucc_optimizer.zero_grad()\n",
    "\n",
    "                # do optimizer step and zerograd for rcc model\n",
    "                self.rcc_optimizer.step()\n",
    "                self.rcc_optimizer.zero_grad()\n",
    "\n",
    "                # scheduler update (remove if it doesnt work!)\n",
    "                self.ae_scheduler.step()\n",
    "                self.ucc_scheduler.step()\n",
    "                self.rcc_scheduler.step()\n",
    "\n",
    "                # add to epoch batch_loss\n",
    "                epoch_training_loss += batch_loss.item()\n",
    "                epoch_ae_loss += ae_loss.item()\n",
    "                epoch_ucc_loss += ucc_loss.item()\n",
    "                epoch_rcc_loss += rcc_loss.item()\n",
    "\n",
    "                # Update the epoch progress bar (overwrite in place)\n",
    "                batch_stats = {\n",
    "                    \"batch_loss\": batch_loss.item(),\n",
    "                    \"ae_loss\": ae_loss.item(),\n",
    "                    \"ucc_loss\": ucc_loss.item(),\n",
    "                    \"rcc_loss\": rcc_loss.item(),\n",
    "                    \"batch_ucc_acc\": batch_ucc_accuracy,\n",
    "                    \"batch_rcc_acc\": batch_rcc_accuracy\n",
    "                }\n",
    "\n",
    "                epoch_progress_bar.set_postfix(batch_stats)\n",
    "                epoch_progress_bar.update(1)\n",
    "\n",
    "            # close the epoch progress bar\n",
    "            epoch_progress_bar.close()\n",
    "\n",
    "            # calculate average epoch train statistics\n",
    "            avg_train_stats = self.calculate_avg_train_stats_hook(epoch_training_loss, epoch_ae_loss, epoch_ucc_loss,\n",
    "                                                                  epoch_rcc_loss)\n",
    "\n",
    "            # calculate validation statistics\n",
    "            avg_val_stats = self.validation_hook()\n",
    "\n",
    "            # Store running history\n",
    "            self.store_running_history_hook(epoch, avg_train_stats, avg_val_stats)\n",
    "\n",
    "            # Show epoch stats\n",
    "            print(f\"# Epoch {epoch + 1}\")\n",
    "            epoch_postfix = self.calculate_and_print_epoch_stats_hook(avg_train_stats, avg_val_stats)\n",
    "\n",
    "            # Update the total progress bar\n",
    "            total_progress_bar.set_postfix(epoch_postfix)\n",
    "\n",
    "            # Close tqdm bar\n",
    "            total_progress_bar.update(1)\n",
    "\n",
    "            # Save model checkpoint periodically\n",
    "            need_to_save_model_checkpoint = (epoch + 1) % epoch_saver_count == 0\n",
    "            if need_to_save_model_checkpoint:\n",
    "                print(f\"Going to save model {self.name} @ Epoch:{epoch + 1}\")\n",
    "                self.save_model_checkpoint_hook(epoch)\n",
    "\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # Close the total progress bar\n",
    "        total_progress_bar.close()\n",
    "\n",
    "        # Return the current state\n",
    "        return self.get_current_running_history_state_hook()\n",
    "\n",
    "    # hooks\n",
    "    # DONE\n",
    "    def init_params_from_checkpoint_hook(self, load_from_checkpoint, resume_epoch_num):\n",
    "        if load_from_checkpoint:\n",
    "            # NOTE: resume_epoch_num can be None here if we want to load from the most recently saved checkpoint!\n",
    "            checkpoint_path = self.get_model_checkpoint_path(resume_epoch_num)\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "            # load previous state of models\n",
    "            self.autoencoder_model.load_state_dict(checkpoint['ae_model_state_dict'])\n",
    "            self.ucc_predictor_model.load_state_dict(checkpoint['ucc_model_state_dict'])\n",
    "            self.rcc_predictor_model.load_state_dict(checkpoint['rcc_model_state_dict'])\n",
    "\n",
    "            # load previous state of optimizers\n",
    "            self.ae_optimizer.load_state_dict(checkpoint['ae_optimizer_state_dict'])\n",
    "            self.ucc_optimizer.load_state_dict(checkpoint['ucc_optimizer_state_dict'])\n",
    "            self.rcc_optimizer.load_state_dict(checkpoint['rcc_optimizer_state_dict'])\n",
    "\n",
    "            # Things we are keeping track of\n",
    "            self.start_epoch = checkpoint['epoch']\n",
    "            self.epoch_numbers = checkpoint['epoch_numbers']\n",
    "\n",
    "            self.training_losses = checkpoint['training_losses']\n",
    "            self.training_ae_losses = checkpoint['training_ae_losses']\n",
    "            self.training_ucc_losses = checkpoint['training_ucc_losses']\n",
    "            self.training_rcc_losses = checkpoint['training_rcc_losses']\n",
    "            self.training_ucc_accuracies = checkpoint['training_ucc_accuracies']\n",
    "            self.training_rcc_accuracies = checkpoint['training_rcc_accuracies']\n",
    "\n",
    "            self.val_losses = checkpoint['val_losses']\n",
    "            self.val_ae_losses = checkpoint['val_ae_losses']\n",
    "            self.val_ucc_losses = checkpoint['val_ucc_losses']\n",
    "            self.val_rcc_losses = checkpoint['val_rcc_losses']\n",
    "            self.val_ucc_accuracies = checkpoint['val_ucc_accuracies']\n",
    "            self.val_rcc_accuracies = checkpoint['val_rcc_accuracies']\n",
    "\n",
    "            print(f\"Model checkpoint for {self.name} is loaded from {checkpoint_path}!\")\n",
    "\n",
    "    # DONE\n",
    "    def init_scheduler_hook(self, num_epochs):\n",
    "        # steps per epoch here is multiplied with bag size as we are doing it at an image level\n",
    "        self.ae_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.ae_optimizer,\n",
    "            config.learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "            # steps_per_epoch=len(self.train_loader) * config.bag_size # this is only if I decide to go image by image level loss as opposed to bag level loss\n",
    "        )\n",
    "\n",
    "        # here we are doing it at a bag level\n",
    "        self.ucc_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.ucc_optimizer,\n",
    "            config.learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "        )\n",
    "\n",
    "        # here we are doing it at a bag level\n",
    "        self.rcc_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.rcc_optimizer,\n",
    "            config.learning_rate,\n",
    "            epochs=num_epochs,\n",
    "            steps_per_epoch=len(self.train_loader)\n",
    "        )\n",
    "\n",
    "    # DONE\n",
    "    def forward_propagate_autoencoder(self, images):\n",
    "        # data is of shape (batchsize=2,bag=10,channels=3,height=32,width=32)\n",
    "        # generally batch size of 16 is good for cifar10 so predicting 20 won't be so bad\n",
    "        batch_size, bag_size, num_channels, height, width = images.size()\n",
    "        batches_of_bag_images = images.view(batch_size * bag_size, num_channels, height, width)\n",
    "        encoded, decoded = self.autoencoder_model(\n",
    "            batches_of_bag_images)  # we are feeding in Batch*bag images of shape (3,32,32)\n",
    "        ae_loss = self.ae_loss_criterion(decoded, batches_of_bag_images)  # compares (Batch * Bag, 3,32,32)\n",
    "        return ae_loss, encoded, decoded\n",
    "\n",
    "    # DONE\n",
    "    def forward_propogate_ucc(self, decoded, one_hot_ucc_labels, is_train_mode=True):\n",
    "        # decoded is of shape [Batch * Bag, 48*16] ->  make it into shape [Batch, Bag, 48*16]\n",
    "        batch_times_bag_size, feature_size = decoded.size()\n",
    "        bag_size = config.bag_size\n",
    "        batch_size = batch_times_bag_size // bag_size\n",
    "        decoded = decoded.view(batch_size, bag_size, feature_size)\n",
    "        ucc_logits = self.ucc_predictor_model(decoded)\n",
    "\n",
    "        # compute the ucc_loss\n",
    "        ucc_loss = self.ucc_loss_criterion(ucc_logits, one_hot_ucc_labels)\n",
    "\n",
    "        # compute the batch stats right here and save it\n",
    "        ucc_probs = nn.Softmax(dim=1)(ucc_logits)\n",
    "        predicted = torch.argmax(ucc_probs, 1)\n",
    "        labels = torch.argmax(one_hot_ucc_labels, 1)\n",
    "        batch_correct_predictions = (predicted == labels).sum().item()\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # calculate batchwise accuracy/ucc_loss\n",
    "        batch_ucc_accuracy = batch_correct_predictions / batch_size\n",
    "        if is_train_mode:\n",
    "            self.train_ucc_correct_predictions += batch_correct_predictions\n",
    "            self.train_ucc_total_batches += batch_size\n",
    "        else:\n",
    "            self.eval_ucc_correct_predictions += batch_correct_predictions\n",
    "            self.eval_ucc_total_batches += batch_size\n",
    "        return ucc_loss, batch_ucc_accuracy\n",
    "\n",
    "    # DONE\n",
    "    '''\n",
    "    NOTE: To improve this I can also add a rcc-ucc-enforcement loss where the number of unique classes should match the ucc exactly\n",
    "    '''\n",
    "\n",
    "    def forward_propogate_rcc(self, decoded, rcc_labels, is_train_mode=True):\n",
    "        # decoded is of shape [Batch * Bag, 48*16] ->  make it into shape [Batch, Bag, 48*16]\n",
    "        batch_times_bag_size, feature_size = decoded.size()\n",
    "        bag_size = config.bag_size\n",
    "        batch_size = batch_times_bag_size // bag_size\n",
    "        decoded = decoded.view(batch_size, bag_size, feature_size)\n",
    "        rcc_logits = self.rcc_predictor_model(decoded)\n",
    "\n",
    "        # round it to the nearest integer\n",
    "        predicted = torch.round(rcc_logits)\n",
    "\n",
    "        # compute the rcc_loss\n",
    "        rcc_loss = self.rcc_loss_criterion(rcc_logits, rcc_labels)\n",
    "\n",
    "        # compute the batch stats right here and save it\n",
    "        batch_size = rcc_labels.size(0)\n",
    "\n",
    "        # NOTE: not sure if it is dim\n",
    "        batch_correct_predictions = (predicted == rcc_labels).sum(dim=1).item()\n",
    "\n",
    "        # calculate batchwise accuracy/ucc_loss\n",
    "        batch_rcc_accuracy = batch_correct_predictions / batch_size\n",
    "        if is_train_mode:\n",
    "            self.train_rcc_correct_predictions += batch_correct_predictions\n",
    "            self.train_rcc_total_batches += batch_size\n",
    "        else:\n",
    "            self.eval_rcc_correct_predictions += batch_correct_predictions\n",
    "            self.eval_rcc_total_batches += batch_size\n",
    "        return rcc_loss, batch_rcc_accuracy\n",
    "\n",
    "    # DONE\n",
    "    def calculate_avg_train_stats_hook(self, epoch_training_loss, epoch_ae_loss, epoch_ucc_loss, epoch_rcc_loss):\n",
    "        avg_training_loss_for_epoch = epoch_training_loss / len(self.train_loader)\n",
    "        avg_ae_loss_for_epoch = epoch_ae_loss / len(self.train_loader)\n",
    "        avg_ucc_loss_for_epoch = epoch_ucc_loss / len(self.train_loader)\n",
    "        avg_rcc_loss_for_epoch = epoch_rcc_loss / len(self.train_loader)\n",
    "        avg_ucc_training_accuracy = self.train_ucc_correct_predictions / self.train_ucc_total_batches\n",
    "        avg_rcc_training_accuracy = self.train_rcc_correct_predictions / self.train_rcc_total_batches\n",
    "\n",
    "        epoch_train_stats = {\n",
    "            \"avg_training_loss\": avg_training_loss_for_epoch,\n",
    "            \"avg_ae_loss\": avg_ae_loss_for_epoch,\n",
    "            \"avg_ucc_loss\": avg_ucc_loss_for_epoch,\n",
    "            \"avg_rcc_loss\": avg_rcc_loss_for_epoch,\n",
    "            \"avg_ucc_training_accuracy\": avg_ucc_training_accuracy,\n",
    "            \"avg_rcc_training_accuracy\": avg_rcc_training_accuracy\n",
    "        }\n",
    "\n",
    "        # reset\n",
    "        self.train_ucc_correct_predictions = 0\n",
    "        self.train_ucc_total_batches = 0\n",
    "\n",
    "        self.train_rcc_correct_predictions = 0\n",
    "        self.train_rcc_total_batches = 0\n",
    "\n",
    "        return epoch_train_stats\n",
    "\n",
    "    # DONE\n",
    "    def validation_hook(self):\n",
    "        # class level init\n",
    "        self.eval_ucc_correct_predictions = 0\n",
    "        self.eval_ucc_total_batches = 0\n",
    "\n",
    "        self.eval_rcc_correct_predictions = 0\n",
    "        self.eval_rcc_total_batches = 0\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_ae_loss = 0.0\n",
    "        val_ucc_loss = 0.0\n",
    "        val_rcc_loss = 0.0\n",
    "\n",
    "        # set all models to eval mode\n",
    "        self.autoencoder_model.eval()\n",
    "        self.ucc_predictor_model.eval()\n",
    "        self.rcc_predictor_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_batch_idx, val_data in enumerate(self.val_loader):\n",
    "                val_images, val_one_hot_ucc_labels, val_rcc_labels = val_data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                val_batch_ae_loss, val_encoded, val_decoded = self.forward_propagate_autoencoder(val_images)\n",
    "                val_batch_ucc_loss, val_batch_ucc_accuracy = self.forward_propogate_ucc(val_decoded,\n",
    "                                                                                        val_one_hot_ucc_labels, False)\n",
    "                val_batch_rcc_loss, val_batch_rcc_accuracy = self.forward_propogate_rcc(val_decoded, val_rcc_labels,\n",
    "                                                                                        False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                val_batch_loss = val_batch_ae_loss + val_batch_ucc_loss + val_batch_rcc_loss\n",
    "\n",
    "                # cummulate the losses\n",
    "                val_loss += val_batch_loss\n",
    "                val_ae_loss += val_batch_ae_loss\n",
    "                val_ucc_loss += val_batch_ucc_loss\n",
    "                val_rcc_loss += val_batch_rcc_loss\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_val_loss = val_loss / len(self.val_loader)\n",
    "        avg_val_ucc_loss = val_ucc_loss / len(self.val_loader)\n",
    "        avg_val_rcc_loss = val_rcc_loss / len(self.val_loader)\n",
    "        avg_val_ae_loss = val_ae_loss / len(self.val_loader)\n",
    "        avg_val_ucc_training_accuracy = self.eval_ucc_correct_predictions / self.eval_ucc_total_batches\n",
    "        avg_val_rcc_training_accuracy = self.eval_rcc_correct_predictions / self.eval_rcc_total_batches\n",
    "\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.val_loader)\n",
    "\n",
    "        return {\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"avg_val_ae_loss\": avg_val_ae_loss,\n",
    "            \"avg_val_ucc_loss\": avg_val_ucc_loss,\n",
    "            \"avg_val_rcc_loss\": avg_val_rcc_loss,\n",
    "            \"avg_val_ucc_training_accuracy\": avg_val_ucc_training_accuracy,\n",
    "            \"avg_val_rcc_training_accuracy\": avg_val_rcc_training_accuracy\n",
    "        }\n",
    "\n",
    "    # DONE\n",
    "    def calculate_and_print_epoch_stats_hook(self, avg_train_stats, avg_val_stats):\n",
    "        epoch_loss = avg_train_stats[\"avg_training_loss\"]\n",
    "        epoch_ae_loss = avg_train_stats[\"avg_ae_loss\"]\n",
    "        epoch_ucc_loss = avg_train_stats[\"avg_ucc_loss\"]\n",
    "        epoch_rcc_loss = avg_train_stats[\"avg_rcc_loss\"]\n",
    "        epoch_ucc_accuracy = avg_train_stats[\"avg_ucc_training_accuracy\"]\n",
    "        epoch_rcc_accuracy = avg_train_stats[\"avg_rcc_training_accuracy\"]\n",
    "\n",
    "        epoch_val_loss = avg_val_stats[\"avg_val_loss\"]\n",
    "        epoch_val_ae_loss = avg_val_stats[\"avg_val_ae_loss\"]\n",
    "        epoch_val_ucc_loss = avg_val_stats[\"avg_val_ucc_loss\"]\n",
    "        epoch_val_rcc_loss = avg_val_stats[\"avg_val_rcc_loss\"]\n",
    "        epoch_val_ucc_accuracy = avg_val_stats[\"avg_val_ucc_training_accuracy\"]\n",
    "        epoch_val_rcc_accuracy = avg_val_stats[\"avg_val_rcc_training_accuracy\"]\n",
    "\n",
    "        print(\n",
    "            f\"[TRAIN]: Epoch Loss: {epoch_loss} | AE Loss: {epoch_ae_loss} | UCC Loss: {epoch_ucc_loss} | UCC Acc: {epoch_ucc_accuracy} | RCC Loss: {epoch_rcc_loss} | RCC Acc: {epoch_rcc_accuracy}\")\n",
    "        print(\n",
    "            f\"[VAL]: Val Loss: {epoch_val_loss} | Val AE Loss: {epoch_val_ae_loss} | Val UCC Loss: {epoch_val_ucc_loss} | Val UCC Acc: {epoch_val_ucc_accuracy} | Val RCC Loss: {epoch_val_rcc_loss} | Val RCC Acc: {epoch_val_rcc_accuracy}\")\n",
    "\n",
    "        return {\n",
    "            \"epoch_loss\": epoch_loss,\n",
    "            \"epoch_ae_loss\": epoch_ae_loss,\n",
    "            \"epoch_ucc_loss\": epoch_ucc_loss,\n",
    "            \"epoch_rcc_loss\": epoch_rcc_loss,\n",
    "            \"epoch_ucc_acc\": epoch_ucc_accuracy,\n",
    "            \"epoch_rcc_acc\": epoch_rcc_accuracy,\n",
    "            \"epoch_val_loss\": epoch_val_loss,\n",
    "            \"epoch_val_ae_loss\": epoch_val_ae_loss,\n",
    "            \"epoch_val_ucc_loss\": epoch_val_ucc_loss,\n",
    "            \"epoch_val_rcc_loss\": epoch_val_rcc_loss,\n",
    "            \"epoch_val_ucc_acc\": epoch_val_ucc_accuracy,\n",
    "            \"epoch_val_rcc_acc\": epoch_val_rcc_accuracy\n",
    "        }\n",
    "\n",
    "    # DONE\n",
    "    def store_running_history_hook(self, epoch, avg_train_stats, avg_val_stats):\n",
    "        self.epoch_numbers.append(epoch + 1)\n",
    "\n",
    "        self.training_ae_losses.append(avg_train_stats[\"avg_ae_loss\"])\n",
    "        self.training_ucc_losses.append(avg_train_stats[\"avg_ucc_loss\"])\n",
    "        self.training_rcc_losses.append(avg_train_stats[\"avg_rcc_loss\"])\n",
    "        self.training_losses.append(avg_train_stats[\"avg_training_loss\"])\n",
    "        self.training_ucc_accuracies.append(avg_train_stats[\"avg_ucc_training_accuracy\"])\n",
    "        self.training_rcc_accuracies.append(avg_train_stats[\"avg_rcc_training_accuracy\"])\n",
    "\n",
    "        self.val_ae_losses.append(avg_val_stats[\"avg_val_ae_loss\"])\n",
    "        self.val_ucc_losses.append(avg_val_stats[\"avg_val_ucc_loss\"])\n",
    "        self.val_rcc_losses.append(avg_val_stats[\"avg_val_rcc_loss\"])\n",
    "        self.val_losses.append(avg_val_stats[\"avg_val_loss\"])\n",
    "        self.val_ucc_accuracies.append(avg_val_stats[\"avg_val_ucc_training_accuracy\"])\n",
    "        self.val_rcc_accuracies.append(avg_val_stats[\"avg_val_rcc_training_accuracy\"])\n",
    "\n",
    "    # DONE\n",
    "    def get_current_running_history_state_hook(self):\n",
    "        return self.epoch_numbers, \\\n",
    "            self.training_ae_losses, self.training_ucc_losses, self.training_rcc_losses, self.training_losses, self.training_ucc_accuracies, self.training_rcc_accuracies, \\\n",
    "            self.val_ae_losses, self.val_ucc_losses, self.val_rcc_losses, self.val_losses, self.val_ucc_accuracies, self.val_rcc_accuracies\n",
    "\n",
    "    # DONE\n",
    "    def save_model_checkpoint_hook(self, epoch):\n",
    "        # set it to train mode to save the weights (but doesn't matter apparently!)\n",
    "        self.autoencoder_model.train()\n",
    "        self.ucc_predictor_model.train()\n",
    "        self.rcc_predictor_model.train()\n",
    "\n",
    "        # create the directory if it doesn't exist\n",
    "        model_save_directory = os.path.join(self.save_dir, self.name)\n",
    "        os.makedirs(model_save_directory, exist_ok=True)\n",
    "\n",
    "        # Checkpoint the model at the end of each epoch\n",
    "        checkpoint_path = os.path.join(model_save_directory, f'model_epoch_{epoch + 1}.pt')\n",
    "        torch.save(\n",
    "            {\n",
    "                'ae_model_state_dict': self.autoencoder_model.state_dict(),\n",
    "                'ucc_model_state_dict': self.ucc_predictor_model.state_dict(),\n",
    "                'rcc_model_state_dict': self.rcc_predictor_model.state_dict(),\n",
    "                'ae_optimizer_state_dict': self.ae_optimizer.state_dict(),\n",
    "                'ucc_optimizer_state_dict': self.ucc_optimizer.state_dict(),\n",
    "                'rcc_optimizer_state_dict': self.rcc_optimizer.state_dict(),\n",
    "                'epoch': epoch + 1,\n",
    "                'epoch_numbers': self.epoch_numbers,\n",
    "                'training_losses': self.training_losses,\n",
    "                'training_ae_losses': self.training_ae_losses,\n",
    "                'training_ucc_losses': self.training_ucc_losses,\n",
    "                'training_rcc_losses': self.training_rcc_losses,\n",
    "                'training_ucc_accuracies': self.training_ucc_accuracies,\n",
    "                'training_rcc_accuracies': self.training_rcc_accuracies,\n",
    "                'val_losses': self.val_losses,\n",
    "                'val_ae_losses': self.val_ae_losses,\n",
    "                'val_ucc_losses': self.val_ucc_losses,\n",
    "                'val_rcc_losses': self.val_rcc_losses,\n",
    "                'val_ucc_accuracies': self.val_ucc_accuracies,\n",
    "                'val_rcc_accuracies': self.val_rcc_accuracies\n",
    "            },\n",
    "            checkpoint_path\n",
    "        )\n",
    "        print(f\"Saved the model checkpoint for experiment {self.name} for epoch {epoch + 1}\")\n",
    "\n",
    "    # DONE\n",
    "    # find the most recent file and return the path\n",
    "    def get_model_checkpoint_path(self, epoch_num=None):\n",
    "        directory = os.path.join(self.save_dir, self.name)\n",
    "        if epoch_num == None:\n",
    "            # Get a list of all files in the directory\n",
    "            files = os.listdir(directory)\n",
    "\n",
    "            # Filter out only the files (exclude directories)\n",
    "            files = [f for f in files if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "            # Sort the files by their modification time in descending order (most recent first)\n",
    "            files.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)\n",
    "\n",
    "            # Get the name of the most recently added file\n",
    "            model_file = files[0] if files else None\n",
    "        else:\n",
    "            model_file = f\"model_epoch_{epoch_num}.pt\"\n",
    "        return os.path.join(directory, model_file)\n",
    "\n",
    "    # DONE\n",
    "    def show_sample_reconstructions(self, dataloader):\n",
    "        self.autoencoder_model.eval()\n",
    "\n",
    "        # Create a subplot grid\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(9, 9))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_data in dataloader:\n",
    "                val_images, _, _ = val_data\n",
    "\n",
    "                batch_size, bag_size, num_channels, height, width = val_images.size()\n",
    "                bag_val_images = val_images.view(batch_size * bag_size, num_channels, height, width)\n",
    "\n",
    "                # Forward pass through the model\n",
    "                _, _, val_reconstructed_images = self.forward_propagate_autoencoder(val_images)\n",
    "\n",
    "                # take only one image from the bag\n",
    "                sample_image = bag_val_images[0]\n",
    "                predicted_image = val_reconstructed_images[0]\n",
    "\n",
    "                # get it to cpu\n",
    "                sample_image = sample_image.to(\"cpu\")\n",
    "                predicted_image = predicted_image.to(\"cpu\")\n",
    "\n",
    "                # convert to PIL Image\n",
    "                sample_image = self.tensor_to_img_transform(sample_image)\n",
    "                predicted_image = self.tensor_to_img_transform(predicted_image)\n",
    "\n",
    "                axes[0].imshow(sample_image)\n",
    "                axes[0].set_title(f\"Sample Original Image\", color='green')\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                axes[1].imshow(predicted_image)\n",
    "                axes[1].set_title(f\"Sample Reconstructed Image\", color='red')\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                # show only one image\n",
    "                break\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # TODO.x\n",
    "    def test_model(self):\n",
    "        # class level init\n",
    "        self.eval_ucc_correct_predictions = 0\n",
    "        self.eval_ucc_total_batches = 0\n",
    "\n",
    "        self.eval_rcc_correct_predictions = 0\n",
    "        self.eval_rcc_total_batches = 0\n",
    "\n",
    "        test_loss = 0.0\n",
    "        test_ae_loss = 0.0\n",
    "        test_ucc_loss = 0.0\n",
    "        test_rcc_loss = 0.0\n",
    "\n",
    "        # set all models to eval mode\n",
    "        self.autoencoder_model.eval()\n",
    "        self.ucc_predictor_model.eval()\n",
    "        self.rcc_predictor_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_batch_idx, test_data in enumerate(self.test_loader):\n",
    "                test_images, test_one_hot_ucc_labels, test_rcc_labels = test_data\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                test_batch_ae_loss, test_encoded, test_decoded = self.forward_propagate_autoencoder(test_images)\n",
    "                test_batch_ucc_loss, test_batch_ucc_accuracy = self.forward_propogate_ucc(test_decoded,\n",
    "                                                                                        test_one_hot_ucc_labels, False)\n",
    "                test_batch_rcc_loss, test_batch_rcc_accuracy = self.forward_propogate_rcc(test_decoded, test_rcc_labels,\n",
    "                                                                                        False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                test_batch_loss = test_batch_ae_loss + test_batch_ucc_loss + test_batch_rcc_loss\n",
    "\n",
    "                # cummulate the losses\n",
    "                test_loss += test_batch_loss\n",
    "                test_ae_loss += test_batch_ae_loss\n",
    "                test_ucc_loss += test_batch_ucc_loss\n",
    "                test_rcc_loss += test_batch_rcc_loss\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_test_loss = test_loss / len(self.test_loader)\n",
    "        avg_test_ucc_loss = test_ucc_loss / len(self.test_loader)\n",
    "        avg_test_rcc_loss = test_rcc_loss / len(self.test_loader)\n",
    "        avg_test_ae_loss = test_ae_loss / len(self.test_loader)\n",
    "        avg_test_ucc_training_accuracy = self.eval_ucc_correct_predictions / self.eval_ucc_total_batches\n",
    "        avg_test_rcc_training_accuracy = self.eval_rcc_correct_predictions / self.eval_rcc_total_batches\n",
    "\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.test_loader)\n",
    "\n",
    "        return {\n",
    "            \"avg_test_loss\": avg_test_loss,\n",
    "            \"avg_test_ae_loss\": avg_test_ae_loss,\n",
    "            \"avg_test_ucc_loss\": avg_test_ucc_loss,\n",
    "            \"avg_test_rcc_loss\": avg_test_rcc_loss,\n",
    "            \"avg_test_ucc_training_accuracy\": avg_test_ucc_training_accuracy,\n",
    "            \"avg_test_rcc_training_accuracy\": avg_test_rcc_training_accuracy\n",
    "        }\n",
    "\n",
    "    # DONE\n",
    "    def js_divergence(self, p, q):\n",
    "        \"\"\"\n",
    "        Calculate the Jensen-Shannon Divergence between two probability distributions p and q.\n",
    "\n",
    "        Args:\n",
    "        p (torch.Tensor): Probability distribution p.\n",
    "        q (torch.Tensor): Probability distribution q.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Jensen-Shannon Divergence between p and q.\n",
    "        \"\"\"\n",
    "        # Calculate the average distribution 'm'\n",
    "        m = 0.5 * (p + q)\n",
    "\n",
    "        # Calculate the KL Divergence of 'p' and 'q' from 'm'\n",
    "        kl_div_p = F.kl_div(p.log(), m, reduction='batchmean')\n",
    "        kl_div_q = F.kl_div(q.log(), m, reduction='batchmean')\n",
    "\n",
    "        # Compute the JS Divergence\n",
    "        js_divergence = 0.5 * (kl_div_p + kl_div_q)\n",
    "\n",
    "        return js_divergence\n",
    "\n",
    "    # DONE\n",
    "    def calculate_min_js_divergence(self):\n",
    "        num_classes = len(self.kde_loaders)\n",
    "        kde_per_class = {class_idx: 0.0 for class_idx in range(num_classes)}\n",
    "\n",
    "        # find the average kde across all classes\n",
    "        for class_idx, pure_class_kde_loader in tqdm(enumerate(self.kde_loaders)):\n",
    "            num_imgs_in_class = 0\n",
    "            for batch_idx, images in tqdm(enumerate(pure_class_kde_loader)):\n",
    "                # batch data is of shape ( Batch,bag, 3,32,32)\n",
    "                batch_size, bag_size, num_channels, height, width = images.size()\n",
    "                # reshaping to shape ( batch * bag, 3 ,32,32)\n",
    "                batches_of_bag_images = images.view(batch_size * bag_size, num_channels, height, width)\n",
    "                latent_features = self.autoencoder_model.encoder(batches_of_bag_images)  # shape (Batch * bag, 48*16)\n",
    "                batch_kde_distributions = self.ucc_predictor_model.kde(latent_features)  # shape [Batch=2, 8448]\n",
    "                num_imgs_in_class += batch_kde_distributions.size(0)\n",
    "                kde_distributions = torch.sum(batch_kde_distributions, dim=0)\n",
    "                kde_per_class[class_idx] += kde_distributions\n",
    "            kde_per_class[class_idx] /= num_imgs_in_class\n",
    "\n",
    "        # find the js_divergence\n",
    "        min_divergence = torch.inf\n",
    "        best_i = None\n",
    "        best_j = None\n",
    "        for i in range(num_classes):\n",
    "            for j in range(i + 1, num_classes):\n",
    "                divergence = self.js_divergence(kde_per_class[i], kde_per_class[j])\n",
    "                print(f\"JS Divergence between {i} & {j} is {divergence}\")\n",
    "                if divergence < min_divergence:\n",
    "                    min_divergence = divergence\n",
    "                    best_i = i\n",
    "                    best_j = j\n",
    "\n",
    "        print(f\"Min JS Divergence is {min_divergence} between classes {best_i} & {best_j}\")\n",
    "        # return the min divergence\n",
    "        return min_divergence\n",
    "\n",
    "    # DONE\n",
    "    def calculate_clustering_accuracy(self):\n",
    "        all_latent_features = []\n",
    "        truth_labels_arr = []\n",
    "        for pure_autoencoder_loader in self.autoencoder_loaders:\n",
    "            for batch_idx, data in tqdm(enumerate(pure_autoencoder_loader)):\n",
    "                # batch data is of shape (1,3,32,32), (1,1)\n",
    "                image, label = data\n",
    "                latent_features = self.autoencoder_model.encoder(image)  # shape (1, 48*16)\n",
    "\n",
    "                latent_features = latent_features.squeeze().numpy()  # ndarray shape (48*16)\n",
    "                label = label.squeeze().numpy()  # ndarray shape (1)\n",
    "\n",
    "                all_latent_features.append(latent_features)\n",
    "                truth_labels_arr.append(label)\n",
    "\n",
    "        all_latent_features = np.array(all_latent_features)\n",
    "\n",
    "        # Do kmeans fit\n",
    "        estimator = KMeans(n_clusters=10, init='k-means++', n_init=10)\n",
    "        estimator.fit(all_latent_features)\n",
    "        predicted_clustering_labels = estimator.labels_\n",
    "\n",
    "        # Calculate accuracy\n",
    "        cost_matrix = np.zeros((10, 10))\n",
    "        num_samples = np.zeros(10)\n",
    "        for truth_val in range(10):\n",
    "            temp_sample_indices = np.where(truth_labels_arr == truth_val)[0]\n",
    "            num_samples[truth_val] = temp_sample_indices.shape[0]\n",
    "\n",
    "            temp_predicted_labels = predicted_clustering_labels[temp_sample_indices]\n",
    "\n",
    "            for predicted_val in range(10):\n",
    "                temp_matching_pairs = np.where(temp_predicted_labels == predicted_val)[0]\n",
    "                cost_matrix[truth_val, predicted_val] = 1 - (\n",
    "                        temp_matching_pairs.shape[0] / temp_sample_indices.shape[0])\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        cost = cost_matrix[row_ind, col_ind]\n",
    "\n",
    "        clustering_acc = ((1 - cost) * num_samples).sum() / num_samples.sum()\n",
    "        return clustering_acc\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
