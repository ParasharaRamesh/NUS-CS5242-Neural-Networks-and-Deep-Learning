{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installing python modules used later on"
   ],
   "metadata": {
    "collapsed": false,
    "id": "SlpsuqkV1iPk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Uncomment if training on google colab!\n",
    "\n",
    "!pip install pytorch_msssim\n",
    "!pip install torchinfo"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lhfdu7AB1iPq",
    "outputId": "c45d4239-b574-4ac0-a0ed-4b01fa3469f2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mount Google Drive"
   ],
   "metadata": {
    "collapsed": false,
    "id": "aPdt08JV1iPt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Uncomment if training on google colab!\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUtNUuoA1iPu",
    "outputId": "0d1200c4-1782-4e59-8e5a-4d14079cbcc9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing relevant things"
   ],
   "metadata": {
    "collapsed": false,
    "id": "MSXaxVQa1iPu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kDt34Hh61iPv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torchinfo import summary\n",
    "from itertools import combinations\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset, ConcatDataset, Dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from pytorch_msssim import SSIM\n",
    "import random\n",
    "from torchvision.models import ResNeXt50_32X4D_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config params"
   ],
   "metadata": {
    "collapsed": false,
    "id": "30WA5eST1iPw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # for local\n",
    "    # datasets_path = f\"../Dataset/splitted_cifar10_dataset.npz\"\n",
    "    # weights_path = f\"../weights\"\n",
    "\n",
    "    drive_path = \"/content/drive/MyDrive\"\n",
    "    datasets_path = f\"{drive_path}/splitted_cifar10_dataset.npz\"\n",
    "    weights_path = f\"{drive_path}/weights\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    sigma = 0.1\n",
    "    num_nodes = 11\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    weight_decay = 1e-5\n",
    "    grad_clip = 1.3\n",
    "\n",
    "    batch_size = 25\n",
    "    ucc_limit = 4\n",
    "    rcc_limit = 10\n",
    "    bag_size = 24\n",
    "    num_classes = 10\n",
    "\n",
    "    train_steps = 30000\n",
    "    test_steps = 1000\n",
    "    val_steps = 100\n",
    "    debug_steps = 1000\n",
    "    saver_steps = 1500\n",
    "\n",
    "\n",
    "config = Config()"
   ],
   "metadata": {
    "id": "xjKkRwyL1iPw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Au0yZcaw1iPx"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "splitted_dataset = np.load(config.datasets_path)\n",
    "\n",
    "x_train = splitted_dataset['x_train']\n",
    "print(f\"x_train shape :{x_train.shape}\")\n",
    "\n",
    "y_train = splitted_dataset['y_train']\n",
    "print(f\"y_train shape :{y_train.shape}\")\n",
    "\n",
    "x_val = splitted_dataset['x_val']\n",
    "print(f\"x_val shape :{x_val.shape}\")\n",
    "\n",
    "y_val = splitted_dataset['y_val']\n",
    "print(f\"y_val shape :{y_val.shape}\")\n",
    "\n",
    "x_test = splitted_dataset['x_test']\n",
    "print(f\"x_test shape :{x_test.shape}\")\n",
    "\n",
    "y_test = splitted_dataset['y_test']\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qt12NdvB1iPx",
    "outputId": "6477de19-05ff-4bd9-ea1f-925db42c9291"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Dataloader\n",
    "\n",
    "This dataloader moves data directly to the device when yielding data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "xbz27xgV1iP1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "Wrapper on top of dataloader to move tensors to device\n",
    "'''\n",
    "class DeviceDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        super().__init__(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in super().__iter__():\n",
    "            yield self._move_to_device(batch)\n",
    "\n",
    "    def _move_to_device(self, batch):\n",
    "        if isinstance(batch, torch.Tensor):\n",
    "            return batch.to(self.device)\n",
    "        elif isinstance(batch, (list, tuple)):\n",
    "            return [self._move_to_device(item) for item in batch]\n",
    "        elif isinstance(batch, dict):\n",
    "            return {key: self._move_to_device(value) for key, value in batch.items()}\n",
    "        else:\n",
    "            return batch\n"
   ],
   "metadata": {
    "id": "aZ3q0u8X1iP2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "id": "SL4Quij91iP2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CIFARDataset(Dataset):\n",
    "    def __init__(self, x, y, num_iter, train_mode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_iter = num_iter\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        self.num_classes = config.num_classes\n",
    "\n",
    "        # for each class get all the indexes of images\n",
    "        self.class_label_to_img_idxs = self.get_class_label_to_img_idxs_dict()\n",
    "\n",
    "        # pick the augmentation based on the mode\n",
    "        if self.train_mode:\n",
    "            self.transforms = [\n",
    "                # normal\n",
    "                transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random horizontal flips\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random rotations\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomRotation(3),\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random rotations & flips\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomRotation(3),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "            ]\n",
    "        else:\n",
    "            self.transforms = [\n",
    "                transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                ])\n",
    "            ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter * config.batch_size\n",
    "\n",
    "    def get_img_from_idx(self, index):\n",
    "        img = self.x[index]\n",
    "        img = Image.fromarray(img)\n",
    "        random_transform = random.choice(self.transforms)\n",
    "        return random_transform(img)\n",
    "\n",
    "    def get_label_from_idx(self, index):\n",
    "        return self.y[index][0]\n",
    "\n",
    "    def get_class_label_to_img_idxs_dict(self):\n",
    "        class_label_to_img_idxs_dict = dict()\n",
    "        for label in range(self.num_classes):\n",
    "            img_idxs = np.where(self.y == label)[0]\n",
    "            class_label_to_img_idxs_dict[label] = img_idxs\n",
    "\n",
    "        return class_label_to_img_idxs_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label = index % self.num_classes\n",
    "        label_img_idxs = self.class_label_to_img_idxs[label]\n",
    "        random_idx = random.choice(label_img_idxs)\n",
    "        return self.get_img_from_idx(random_idx), self.get_label_from_idx(random_idx)\n"
   ],
   "metadata": {
    "id": "N_6_zQada_TB"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RCCDataset(Dataset):\n",
    "    def __init__(self, x, y, num_iter, train_mode):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.num_iter = num_iter\n",
    "        self.train_mode = train_mode\n",
    "\n",
    "        self.bag_size = config.bag_size\n",
    "        self.num_classes = config.num_classes\n",
    "        self.ucc_limit = config.ucc_limit\n",
    "\n",
    "        # pick the augmentation based on the mode\n",
    "        if self.train_mode:\n",
    "            self.transforms = [\n",
    "                # normal\n",
    "                transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random horizontal flips\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random rotations\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomRotation(3),\n",
    "                    transforms.ToTensor()\n",
    "                ]),\n",
    "                # random rotations & flips\n",
    "                transforms.Compose([\n",
    "                    transforms.RandomRotation(3),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor()\n",
    "                ])\n",
    "            ]\n",
    "        else:\n",
    "            self.transforms = [\n",
    "                transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                ])\n",
    "            ]\n",
    "\n",
    "        # for each class get all the indexes of images\n",
    "        self.class_label_to_img_idxs = self.get_class_label_to_img_idxs_dict()\n",
    "\n",
    "        # for each ucc class from (1->4) get all the unique combinations possible\n",
    "        self.ucc_to_all_combos = self.get_ucc_to_all_combinations_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_iter * config.batch_size\n",
    "\n",
    "    def get_class_label_to_img_idxs_dict(self):\n",
    "        class_label_to_img_idxs_dict = dict()\n",
    "        for label_value in range(self.num_classes):\n",
    "            label_key = f\"class{label_value}\"\n",
    "\n",
    "            img_idxs = np.where(self.y == label_value)[0]\n",
    "            class_label_to_img_idxs_dict[label_key] = img_idxs\n",
    "\n",
    "        return class_label_to_img_idxs_dict\n",
    "\n",
    "    def get_ucc_to_all_combinations_dict(self):\n",
    "        class_labels = np.arange(self.num_classes)\n",
    "        ucc_to_all_combos_dict = dict()\n",
    "\n",
    "        for ucc in range(self.ucc_limit):  # go from 1->4\n",
    "            ucc_key = f\"ucc{ucc}\"\n",
    "\n",
    "            ucc_bag_labels = list()\n",
    "            for unique_ucc_combo in combinations(class_labels, ucc+1): #plus 1 here\n",
    "                ucc_bag_labels.append(np.array(unique_ucc_combo))\n",
    "\n",
    "            ucc_to_all_combos_dict[ucc_key] = np.array(ucc_bag_labels)\n",
    "\n",
    "        return ucc_to_all_combos_dict\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ucc_label = index % self.ucc_limit\n",
    "        rcc_label = torch.zeros(self.num_classes)\n",
    "        ucc_combo_with_bag_counts = self.get_random_ucc_combo_and_its_bag_counts(ucc_label)\n",
    "\n",
    "        selected_idxs = []\n",
    "        for label, freq in ucc_combo_with_bag_counts:\n",
    "            label_key = f\"class{label}\"\n",
    "            label_img_idxs = self.class_label_to_img_idxs[label_key]\n",
    "            selected_idxs.extend(list(label_img_idxs[np.random.randint(0, len(label_img_idxs), size=freq)]))\n",
    "            #construct the rcc label\n",
    "            rcc_label[label] = freq\n",
    "\n",
    "        imgs = self.get_imgs_from_idxs(selected_idxs)\n",
    "        return imgs, ucc_label, rcc_label\n",
    "\n",
    "    def get_random_ucc_combo_and_its_bag_counts(self, ucc_label):\n",
    "        class_key = f\"ucc{ucc_label}\"\n",
    "\n",
    "        # Get unique combination of cifar10 labels for ucc label\n",
    "        ucc_bag_labels_list = self.ucc_to_all_combos[class_key]\n",
    "        idx = np.random.randint(0, ucc_bag_labels_list.shape[0])\n",
    "        ucc_labels = ucc_bag_labels_list[idx, :]\n",
    "\n",
    "        # Get even distribution of instances per label with max difference of 1\n",
    "        N = ucc_labels.shape[0]\n",
    "        n_instances = self.bag_size // N\n",
    "\n",
    "        counts = np.repeat(n_instances, N)\n",
    "\n",
    "        res = []\n",
    "        for label, freq in zip(ucc_labels, counts):\n",
    "            res.append((label, freq))\n",
    "        return res\n",
    "\n",
    "    def get_imgs_from_idxs(self, idxs):\n",
    "        imgs = self.x[idxs]\n",
    "        res = []\n",
    "        for i in range(len(imgs)):\n",
    "            img = Image.fromarray(imgs[i])\n",
    "            random_transform = random.choice(self.transforms)\n",
    "            res.append(random_transform(img).unsqueeze(0))\n",
    "        res = torch.concatenate(res, dim=0)\n",
    "        return res"
   ],
   "metadata": {
    "id": "JkfzExFo1iP3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the Dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zh4SdAvF1iP5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dataloaders:\n",
    "    def __init__(self):\n",
    "        data = np.load(config.datasets_path)\n",
    "        x_train, y_train = data[\"x_train\"], data[\"y_train\"]\n",
    "        x_val, y_val = data[\"x_val\"], data[\"y_val\"]\n",
    "        x_test, y_test = data[\"x_test\"], data[\"y_test\"]\n",
    "\n",
    "        #construct rcc datasets\n",
    "        self.rcc_train_dataset = RCCDataset(x_train, y_train, config.train_steps, True)\n",
    "        self.rcc_val_dataset = RCCDataset(x_val, y_val, config.val_steps, False)\n",
    "        self.rcc_test_dataset = RCCDataset(x_test, y_test, config.test_steps, False)\n",
    "\n",
    "        #construct cifar datasets\n",
    "        self.cifar_train_dataset = CIFARDataset(x_train, y_train, config.train_steps, True)\n",
    "        self.cifar_val_dataset = CIFARDataset(x_val, y_val, config.val_steps, False)\n",
    "        self.cifar_test_dataset = CIFARDataset(x_test, y_test, config.test_steps, False)\n",
    "\n",
    "        #delete the extra unnecessary space\n",
    "        del data\n",
    "        del x_train\n",
    "        del x_test\n",
    "        del x_val\n",
    "        del y_train\n",
    "        del y_test\n",
    "        del y_val\n",
    "\n",
    "    def get_rcc_dataloaders(self):\n",
    "        train_loader = DeviceDataLoader(self.rcc_train_dataset, config.batch_size)\n",
    "        val_loader = DeviceDataLoader(self.rcc_val_dataset, config.batch_size)\n",
    "        test_loader = DeviceDataLoader(self.rcc_test_dataset, config.batch_size)\n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    def get_cifar_dataloaders(self):\n",
    "        train_loader = DeviceDataLoader(self.cifar_train_dataset, config.batch_size)\n",
    "        val_loader = DeviceDataLoader(self.cifar_val_dataset, config.batch_size)\n",
    "        test_loader = DeviceDataLoader(self.cifar_test_dataset, config.batch_size)\n",
    "        return train_loader, val_loader, test_loader"
   ],
   "metadata": {
    "id": "vOLdLabva_TC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataloaders = Dataloaders()"
   ],
   "metadata": {
    "id": "Vhk3aDMf1iP6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking how one bag looks like"
   ],
   "metadata": {
    "collapsed": false,
    "id": "D7ynGZWxht_P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#printing the images in a bag\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tensor_to_img_transform = transforms.ToPILImage()\n",
    "rcc_train_loader, rcc_val_loader, rcc_test_loader = dataloaders.get_rcc_dataloaders()\n",
    "rcc_dataloaders = [rcc_train_loader, rcc_val_loader, rcc_test_loader]\n",
    "names = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "for rcc_dataloader, name in zip(rcc_dataloaders, names):\n",
    "    print(f\"Checking out {name}\")\n",
    "    for data in rcc_dataloader:\n",
    "        batches, _, _ = data\n",
    "        for bag in batches:\n",
    "            for image_index, image in enumerate(bag):\n",
    "                image = tensor_to_img_transform(image)\n",
    "                plt.subplot(4, 6, image_index + 1)  # Assuming 24 images per bag\n",
    "                plt.imshow(image)  # Display the image\n",
    "                plt.title(f\"Bag {image_index + 1}\")  # Set the title\n",
    "                plt.axis('off')  # Turn off axis labels\n",
    "            plt.show()\n",
    "            break\n",
    "        break\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aLNJYxByht_P",
    "outputId": "ae4ae698-1e89-49ef-e830-76f17208f0c7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SSIM Loss definition"
   ],
   "metadata": {
    "collapsed": false,
    "id": "A1oDEUBr1iQV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SSIMLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim = SSIM()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Calculate SSIM\n",
    "        ssim_value = self.ssim(x, y)\n",
    "        # Subtract SSIM from 1\n",
    "        loss = 1 - ssim_value\n",
    "        return loss"
   ],
   "metadata": {
    "id": "6l9rTIPl1iQV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model architectures"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Mfu4D_oAht_P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Autoencoder"
   ],
   "metadata": {
    "collapsed": false,
    "id": "t6ZcYlA8ht_P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ResidualZeroPaddingBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            first_block=False,\n",
    "            down_sample=False,\n",
    "            up_sample=False,\n",
    "    ):\n",
    "        super(ResidualZeroPaddingBlock, self).__init__()\n",
    "        self.first_block = first_block\n",
    "        self.down_sample = down_sample\n",
    "        self.up_sample = up_sample\n",
    "\n",
    "        if self.up_sample:\n",
    "            self.upsampling = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            stride=2 if self.down_sample else 1,\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.skip_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=2 if self.down_sample else 1,\n",
    "        )\n",
    "\n",
    "        # Initialize the weights and biases\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        nn.init.constant_(self.conv1.bias, 0.1)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        nn.init.constant_(self.conv2.bias, 0.1)\n",
    "        nn.init.xavier_uniform_(self.skip_conv.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.first_block:\n",
    "            x = nn.LeakyReLU()(x)\n",
    "            if self.up_sample:\n",
    "                x = self.upsampling(x)\n",
    "            out = nn.LeakyReLU()(self.conv1(x))\n",
    "            out = self.conv2(out)\n",
    "            if x.shape != out.shape:\n",
    "                x = self.skip_conv(x)\n",
    "        else:\n",
    "            out = nn.LeakyReLU()(self.conv1(x))\n",
    "            out = nn.LeakyReLU()(self.conv2(out))\n",
    "        return x + out\n",
    "\n",
    "class WideResidualBlocks(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, n, down_sample=False, up_sample=False\n",
    "    ):\n",
    "        super(WideResidualBlocks, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                ResidualZeroPaddingBlock(\n",
    "                    in_channels if i == 0 else out_channels,\n",
    "                    out_channels,\n",
    "                    first_block=(i == 0),\n",
    "                    down_sample=down_sample,\n",
    "                    up_sample=up_sample,\n",
    "                )\n",
    "                for i in range(n)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *target_shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.target_shape)\n",
    "\n",
    "class PretrainedAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pretrained_model = models.resnext50_32x4d(weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1).to(config.device)\n",
    "        self.pretrained_encoder = nn.Sequential(*list(pretrained_model.children())[:-1]).to(config.device)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 128, bias=False),\n",
    "            nn.Dropout(0.13),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.Sigmoid()\n",
    "        ).to(config.device)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            Reshape(*[128, 1, 1]),\n",
    "            WideResidualBlocks(\n",
    "                128,\n",
    "                256,\n",
    "                1,\n",
    "                up_sample=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(256),\n",
    "            WideResidualBlocks(\n",
    "                256,\n",
    "                128,\n",
    "                1,\n",
    "                up_sample=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(128),\n",
    "            WideResidualBlocks(\n",
    "                128,\n",
    "                64,\n",
    "                1,\n",
    "                up_sample=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            WideResidualBlocks(\n",
    "                64,\n",
    "                32,\n",
    "                1,\n",
    "                up_sample=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            WideResidualBlocks(\n",
    "                32,\n",
    "                16,\n",
    "                1,\n",
    "                up_sample=True,\n",
    "            ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            WideResidualBlocks(\n",
    "                16,\n",
    "                8,\n",
    "                1\n",
    "            ),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.Conv2d(\n",
    "                8,\n",
    "                3,\n",
    "                kernel_size=3,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        ).to(config.device)\n",
    "\n",
    "        ## Freeze all the parameters\n",
    "        # for param in self.pretrained_encoder.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        pretrained_features = self.pretrained_encoder(x)\n",
    "        features = self.encoder(pretrained_features).to(config.device)\n",
    "        reconstruction = self.decoder(features)\n",
    "        return features, reconstruction"
   ],
   "metadata": {
    "id": "3Rg0ZO1rht_P"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernel Density Estimator"
   ],
   "metadata": {
    "collapsed": false,
    "id": "wUH15pTxht_P"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KDE(nn.Module):\n",
    "    def __init__(self, device=config.device, num_nodes=config.num_nodes, sigma=config.sigma):\n",
    "        super(KDE, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.sigma = sigma\n",
    "        self.device = device\n",
    "        print(\"KDE Layer initialized\")\n",
    "\n",
    "    def forward(self, data):\n",
    "        batch_size, num_instances, num_features = data.shape\n",
    "\n",
    "        # Create sample points\n",
    "        k_sample_points = (\n",
    "            torch.linspace(0, 1, steps=config.num_nodes)\n",
    "            .repeat(batch_size, num_instances, 1)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        # Calculate constants\n",
    "        k_alpha = 1 / np.sqrt(2 * np.pi * config.sigma ** 2)\n",
    "        k_beta = -1 / (2 * config.sigma ** 2)\n",
    "\n",
    "        # Iterate over features and calculate kernel density estimation for each feature\n",
    "        out_list = []\n",
    "        for i in range(num_features):\n",
    "            one_feature = data[:, :, i: i + 1].repeat(1, 1, config.num_nodes)\n",
    "            k_diff_2 = (k_sample_points - one_feature) ** 2\n",
    "            k_result = k_alpha * torch.exp(k_beta * k_diff_2)\n",
    "            k_out_unnormalized = k_result.sum(dim=1)\n",
    "            k_norm_coeff = k_out_unnormalized.sum(dim=1).view(-1, 1)\n",
    "            k_out = k_out_unnormalized / k_norm_coeff.repeat(\n",
    "                1, k_out_unnormalized.size(1)\n",
    "            )\n",
    "            out_list.append(k_out)\n",
    "\n",
    "        # Concatenate the results\n",
    "        concat_out = torch.cat(out_list, dim=-1).to(self.device)\n",
    "        return concat_out\n"
   ],
   "metadata": {
    "id": "kmp9V314ht_Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RCC Prediction model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "eBlKTOODht_Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RCCModel(nn.Module):\n",
    "    def __init__(self, device=config.device, autoencoder_model=None, ucc_limit=config.ucc_limit, rcc_limit=config.rcc_limit):\n",
    "        super().__init__()\n",
    "        if autoencoder_model:\n",
    "            self.autoencoder = autoencoder_model\n",
    "        else:\n",
    "            self.autoencoder = PretrainedAutoencoder()\n",
    "\n",
    "        self.kde = KDE(device)\n",
    "\n",
    "        self.shared_predictor_stack = nn.Sequential(\n",
    "            nn.Linear(352, 32, bias=False),\n",
    "            nn.Dropout(0.13),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.ucc_predictor = nn.Linear(32, ucc_limit)\n",
    "        self.rcc_predictor = nn.Linear(32, rcc_limit)\n",
    "\n",
    "        print(\"RCC model initialized\")\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Input size: [batch, bag, 3, 32, 32]\n",
    "        # output size: [batch, 4] (ucc_logits), [batch * bag,3,32,32] ( decoded images)\n",
    "\n",
    "        # Stage 1. pass through autoencoder\n",
    "        batch_size, bag_size, num_channels, height, width = batch.size()\n",
    "        batches_of_image_bags = batch.view(batch_size * bag_size, num_channels, height, width).to(torch.float32)\n",
    "        features, decoded = self.autoencoder(\n",
    "            batches_of_image_bags\n",
    "        )  # we are feeding in Batch*bag images of shape (3,32,32)\n",
    "\n",
    "        # Stage 2. use the autoencoder latent features to pass through the ucc predictor\n",
    "        # features shape is now (Batch* Bag, 128) -> (Batch, Bag, 128)\n",
    "        features = features.view(batch_size, bag_size, features.size(1))\n",
    "\n",
    "        # Stage 3. pass through kde to get output shape (Batch, 128*11)\n",
    "        kde_prob_distributions = self.kde(features)\n",
    "\n",
    "        # Stage 4. pass through common stack\n",
    "        common_features = self.shared_predictor_stack(kde_prob_distributions)\n",
    "\n",
    "        # Stage 5. get the ucc logits\n",
    "        ucc_logits = self.ucc_predictor(common_features)\n",
    "\n",
    "        # Stage 6. get the rcc logits\n",
    "        rcc_logits = self.rcc_predictor(common_features)\n",
    "\n",
    "        return rcc_logits, ucc_logits, decoded  # , (Batch , 10), (Batch , 4), (Batch * Bag, 3,32,32)\n",
    "\n",
    "    def get_encoder_features(self, batch):\n",
    "        batch_size, bag_size, num_channels, height, width = batch.size()\n",
    "        batches_of_image_bags = batch.view(batch_size * bag_size, num_channels, height, width).to(torch.float32)\n",
    "        features, _ = self.autoencoder(\n",
    "            batches_of_image_bags\n",
    "        )  # we are feeding in Batch*bag images of shape (3,32,32)\n",
    "        return features\n",
    "\n",
    "    def get_kde_distributions(self, batch):\n",
    "        batch_size, bag_size, num_channels, height, width = batch.size()\n",
    "        batches_of_image_bags = batch.view(batch_size * bag_size, num_channels, height, width).to(torch.float32)\n",
    "        features, _ = self.autoencoder(\n",
    "            batches_of_image_bags\n",
    "        )  # we are feeding in Batch*bag images of shape (3,32,32)\n",
    "        features = features.view(batch_size, bag_size, features.size(1))\n",
    "\n",
    "        kde_prob_distributions = self.kde(features)\n",
    "        return kde_prob_distributions"
   ],
   "metadata": {
    "id": "2sMaUa7Ght_Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RCC model trainable params"
   ],
   "metadata": {
    "collapsed": false,
    "id": "X2scQbfga_TD"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rcc = RCCModel(config.device).to(config.device)\n",
    "summary(rcc, input_size=(config.bag_size, 3, 32, 32), device=config.device, batch_dim=0, col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"], verbose=1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sHDxwkhfa_TD",
    "outputId": "1b222b93-a403-4eab-c914-95ac52911344"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# EXPERIMENT-2 : RCC Model\n",
    "\n",
    "This model is the improvement model where we additionally also try to predict the real class counts as well along with the unique class counts"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qbzZGx0sht_Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code for plotting the model stats"
   ],
   "metadata": {
    "collapsed": false,
    "id": "hoEAbLjGht_Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_ucc_rcc_model_stats(\n",
    "        experiment, epochs,\n",
    "        ucc_training_losses, ae_training_losses, rcc_training_losses, combined_training_losses,\n",
    "        ucc_training_accuracy, rcc_training_accuracy,\n",
    "        ucc_validation_losses, ae_validation_losses, rcc_validation_losses, combined_validation_losses,\n",
    "        ucc_validation_accuracy, rcc_validation_accuracy\n",
    "):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 20))\n",
    "\n",
    "    # Plot training losses\n",
    "    axes[0, 0].plot(epochs, ucc_training_losses, marker=\"o\", color=\"red\", label=\"UCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, ae_training_losses, marker=\"o\", color=\"blue\", label=\"AE Training Loss\")\n",
    "    axes[0, 0].plot(epochs, rcc_training_losses, marker=\"o\", color=\"yellow\", label=\"RCC Training Loss\")\n",
    "    axes[0, 0].plot(epochs, combined_training_losses, marker=\"o\", color=\"green\", label=\"Combined Training Loss\")\n",
    "    axes[0, 0].set_title(f'{experiment}: Training Loss vs Epochs')\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Training Loss')\n",
    "    axes[0, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot training accuracy\n",
    "    axes[0, 1].plot(epochs, ucc_training_accuracy, marker=\"o\", color=\"red\", label=\"UCC Training Accuracy\")\n",
    "    axes[0, 1].plot(epochs, rcc_training_accuracy, marker=\"o\", color=\"green\", label=\"RCC Training Accuracy\")\n",
    "    axes[0, 1].set_title(f'{experiment}: Training Accuracy vs Epochs')\n",
    "    axes[0, 1].set_xlabel('Epochs')\n",
    "    axes[0, 1].set_ylabel('Training Accuracy')\n",
    "    axes[0, 1].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation losses\n",
    "    axes[1, 0].plot(epochs, ucc_validation_losses, marker=\"o\", color=\"red\", label=\"UCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, ae_validation_losses, marker=\"o\", color=\"blue\", label=\"AE Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, rcc_validation_losses, marker=\"o\", color=\"yellow\", label=\"RCC Validation Loss\")\n",
    "    axes[1, 0].plot(epochs, combined_validation_losses, marker=\"o\", color=\"green\", label=\"Combined Validation Loss\")\n",
    "    axes[1, 0].set_title(f'{experiment}: Validation Loss vs Epochs')\n",
    "    axes[1, 0].set_xlabel('Epochs')\n",
    "    axes[1, 0].set_ylabel('Validation Loss')\n",
    "    axes[1, 0].legend()  # Display the legend\n",
    "\n",
    "    # Plot validation accuracy 1,1\n",
    "    axes[1, 1].plot(epochs, ucc_validation_accuracy, marker=\"o\", color=\"red\", label=\"UCC Validation Accuracy\")\n",
    "    axes[1, 1].plot(epochs, rcc_validation_accuracy, marker=\"o\", color=\"green\", label=\"RCC Validation Accuracy\")\n",
    "    axes[1, 1].set_title(f'{experiment}: Validation Accuracy vs Epochs')\n",
    "    axes[1, 1].set_xlabel('Epochs')\n",
    "    axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "    axes[1, 1].legend()  # Display the legend\n",
    "\n",
    "    # Add space between subplots\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    # close it properly\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ],
   "metadata": {
    "id": "IR0_ZMTAht_Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## RCC Trainer class"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Vj5p3PhHht_Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RCCTrainer:\n",
    "    def __init__(self,\n",
    "                 name, rcc_model,\n",
    "                 dataloader, save_dir, device=config.device,\n",
    "                 importances={\"ae\": 0.33, \"ucc\": 0.33, \"rcc\": 0.34},\n",
    "                 use_ssim=False\n",
    "                 ):\n",
    "        self.name = name\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "\n",
    "        # importances\n",
    "        self.ae_importance = importances[\"ae\"]\n",
    "        self.ucc_importance = importances[\"ucc\"]\n",
    "        self.rcc_importance = importances[\"rcc\"]\n",
    "\n",
    "        # dataloaders\n",
    "        self.train_loader, self.val_loader, self.test_loader = dataloader.get_rcc_dataloaders()\n",
    "        self.cifar_train_loader, self.cifar_val_loader, self.cifar_test_loader = dataloader.get_cifar_dataloaders()\n",
    "\n",
    "        # create the directory if it doesn't exist!\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        os.makedirs(os.path.join(self.save_dir, self.name), exist_ok=True)\n",
    "\n",
    "        self.rcc_model = rcc_model\n",
    "\n",
    "        # Adam optimizer(s)\n",
    "        self.rcc_optimizer = optim.Adam(self.rcc_model.parameters(), lr=config.learning_rate,\n",
    "                                        weight_decay=config.weight_decay)\n",
    "\n",
    "        # Loss criterion(s)\n",
    "        self.ae_loss_criterion = SSIMLoss() if use_ssim else nn.MSELoss()\n",
    "        self.ucc_loss_criterion = nn.CrossEntropyLoss()\n",
    "        self.rcc_loss_criterion = nn.MSELoss()\n",
    "\n",
    "        # Transforms\n",
    "        self.tensor_to_img_transform = transforms.ToPILImage()\n",
    "\n",
    "        # Values which can change based on loaded checkpoint\n",
    "        self.steps = []\n",
    "        self.training_losses = []\n",
    "        self.training_ae_losses = []\n",
    "        self.training_ucc_losses = []\n",
    "        self.training_rcc_losses = []\n",
    "        self.training_ucc_accuracies = []\n",
    "        self.training_rcc_accuracies = []\n",
    "\n",
    "        self.val_losses = []\n",
    "        self.val_ae_losses = []\n",
    "        self.val_ucc_losses = []\n",
    "        self.val_rcc_losses = []\n",
    "        self.val_ucc_accuracies = []\n",
    "        self.val_rcc_accuracies = []\n",
    "\n",
    "        self.train_ucc_correct_predictions = 0\n",
    "        self.train_ucc_total_batches = 0\n",
    "\n",
    "        self.train_rcc_correct_predictions = 0\n",
    "        self.train_rcc_total_batches = 0\n",
    "\n",
    "        # Debug saver lists (i.e. capture these stats for every debug_steps)\n",
    "        self.debug_ae_losses = []\n",
    "        self.debug_ucc_losses = []\n",
    "        self.debug_rcc_losses = []\n",
    "        self.debug_total_losses = []\n",
    "\n",
    "    # main train code\n",
    "    def train(self,\n",
    "              resume_steps=None,\n",
    "              load_from_checkpoint=False,\n",
    "              saver_steps=config.saver_steps):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # initialize the params from the saved checkpoint\n",
    "        self.init_params_from_checkpoint_hook(load_from_checkpoint, resume_steps)\n",
    "\n",
    "        # set up scheduler\n",
    "        self.init_scheduler_hook()\n",
    "\n",
    "        # Custom progress bar for each epoch with color\n",
    "        batch_progress_bar = tqdm(\n",
    "            total=len(self.train_loader),\n",
    "            desc=f\"Steps\",\n",
    "            position=1,\n",
    "            leave=False,\n",
    "            dynamic_ncols=True,\n",
    "            ncols=100,\n",
    "            colour='green'\n",
    "        )\n",
    "\n",
    "        # set all models to train mode\n",
    "        self.rcc_model.train()\n",
    "\n",
    "        # iterate over each batch\n",
    "        for step, data in enumerate(self.train_loader):\n",
    "            #zero grad\n",
    "            self.rcc_optimizer.zero_grad()\n",
    "\n",
    "            images, ucc_labels, rcc_labels = data\n",
    "\n",
    "            # forward propogate through the combined model\n",
    "            rcc_logits, ucc_logits, decoded = self.rcc_model(images)\n",
    "\n",
    "            # calculate losses from both models for a batch of bags\n",
    "            ae_loss = self.calculate_autoencoder_loss(images, decoded)\n",
    "            ucc_loss, batch_ucc_accuracy = self.calculate_ucc_loss_and_acc(ucc_logits, ucc_labels, True)\n",
    "            rcc_loss, batch_rcc_accuracy = self.calculate_rcc_loss_and_acc(rcc_logits, rcc_labels, True)\n",
    "\n",
    "            # calculate combined loss\n",
    "            step_loss = (self.ae_importance * ae_loss) + (self.ucc_importance * ucc_loss) + (self.rcc_importance * rcc_loss)\n",
    "\n",
    "            # do loss backward for all losses\n",
    "            step_loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            nn.utils.clip_grad_norm_(self.rcc_model.parameters(), max_norm=config.grad_clip)\n",
    "\n",
    "            # do optimizer step and zerograd\n",
    "            self.rcc_optimizer.step()\n",
    "\n",
    "            # scheduler update (remove if it doesnt work!)\n",
    "            self.rcc_scheduler.step()\n",
    "\n",
    "            # add to epoch batch_loss\n",
    "            self.debug_ae_losses.append(ae_loss.item())\n",
    "            self.debug_ucc_losses.append(ucc_loss.item())\n",
    "            self.debug_rcc_losses.append(rcc_loss.item())\n",
    "            self.debug_total_losses.append(step_loss.item())\n",
    "\n",
    "            # Update the epoch progress bar (overwrite in place)\n",
    "            batch_stats = {\n",
    "                \"batch_loss\": step_loss.item(),\n",
    "                \"batch_ae_loss\": ae_loss.item(),\n",
    "                \"batch_ucc_loss\": ucc_loss.item(),\n",
    "                \"batch_rcc_loss\": rcc_loss.item(),\n",
    "                \"batch_ucc_acc\": batch_ucc_accuracy,\n",
    "                \"batch_rcc_acc\": batch_rcc_accuracy\n",
    "            }\n",
    "\n",
    "            batch_progress_bar.set_postfix(batch_stats)\n",
    "            batch_progress_bar.update(1)\n",
    "\n",
    "            # Compute the average stats for every config.debug_steps steps\n",
    "            if (step + 1) % config.debug_steps == 0:\n",
    "                # calculate average epoch train statistics\n",
    "                avg_train_stats = self.calculate_avg_train_stats_hook()\n",
    "\n",
    "                # calculate validation statistics\n",
    "                avg_val_stats = self.validation_hook()\n",
    "\n",
    "                # Store running history\n",
    "                self.print_stats_and_store_running_history_hook(step + 1, avg_train_stats, avg_val_stats)\n",
    "\n",
    "                # Clear the list\n",
    "                self.debug_ae_losses = []\n",
    "                self.debug_ucc_losses = []\n",
    "                self.debug_rcc_losses = []\n",
    "                self.debug_total_losses = []\n",
    "\n",
    "            # Save model checkpoint periodically\n",
    "            if (step + 1) % saver_steps == 0:\n",
    "                print(f\"Going to save model {self.name} @ Step:{step + 1}\")\n",
    "                self.save_model_checkpoint_hook(step + 1)\n",
    "\n",
    "        # close the epoch progress bar\n",
    "        batch_progress_bar.close()\n",
    "\n",
    "        # Return the current state\n",
    "        return self.get_current_running_history_state_hook()\n",
    "\n",
    "    # hooks\n",
    "    def init_params_from_checkpoint_hook(self, load_from_checkpoint, resume_steps):\n",
    "        if load_from_checkpoint:\n",
    "            # NOTE: resume_epoch_num can be None here if we want to load from the most recently saved checkpoint!\n",
    "            checkpoint_path = self.get_model_checkpoint_path(resume_steps)\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "            # load previous state of models\n",
    "            self.rcc_model.load_state_dict(checkpoint['rcc_model_state_dict'])\n",
    "\n",
    "            # load previous state of optimizers\n",
    "            self.rcc_optimizer.load_state_dict(checkpoint['rcc_optimizer_state_dict'])\n",
    "\n",
    "            # Things we are keeping track of\n",
    "            self.steps = checkpoint['steps']\n",
    "\n",
    "            self.training_losses = checkpoint['training_losses']\n",
    "            self.training_ae_losses = checkpoint['training_ae_losses']\n",
    "            self.training_ucc_losses = checkpoint['training_ucc_losses']\n",
    "            self.training_rcc_losses = checkpoint['training_rcc_losses']\n",
    "            self.training_ucc_accuracies = checkpoint['training_ucc_accuracies']\n",
    "            self.training_rcc_accuracies = checkpoint['training_rcc_accuracies']\n",
    "\n",
    "            self.val_losses = checkpoint['val_losses']\n",
    "            self.val_ae_losses = checkpoint['val_ae_losses']\n",
    "            self.val_ucc_losses = checkpoint['val_ucc_losses']\n",
    "            self.val_rcc_losses = checkpoint['val_rcc_losses']\n",
    "            self.val_ucc_accuracies = checkpoint['val_ucc_accuracies']\n",
    "            self.val_rcc_accuracies = checkpoint['val_rcc_accuracies']\n",
    "\n",
    "            print(f\"Model checkpoint for {self.name} is loaded from {checkpoint_path}!\")\n",
    "\n",
    "    def init_scheduler_hook(self):\n",
    "        self.rcc_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            self.rcc_optimizer,\n",
    "            config.learning_rate,\n",
    "            total_steps=len(self.train_loader)\n",
    "        )\n",
    "\n",
    "    def calculate_autoencoder_loss(self, images, decoded):\n",
    "        # data is of shape (batchsize=2,bag=10,channels=3,height=32,width=32)\n",
    "        # generally batch size of 16 is good for cifar10 so predicting 20 won't be so bad\n",
    "        batch_size, bag_size, num_channels, height, width = images.size()\n",
    "        batches_of_bag_images = images.view(batch_size * bag_size, num_channels, height, width).to(torch.float32)\n",
    "        ae_loss = self.ae_loss_criterion(decoded, batches_of_bag_images)  # compares (Batch * Bag, 3,32,32)\n",
    "        return ae_loss\n",
    "\n",
    "    def calculate_ucc_loss_and_acc(self, ucc_logits, ucc_labels, is_train_mode=True):\n",
    "        # compute the batch stats right here and save it\n",
    "        ucc_probs = nn.Softmax(dim=1)(ucc_logits)\n",
    "        predicted = torch.argmax(ucc_probs, 1)  # class [8,6]\n",
    "        labels = ucc_labels  # class [7,6]\n",
    "        batch_correct_predictions = (predicted == labels).sum().item()  # 0.5\n",
    "        batch_size = labels.size(0)\n",
    "        batch_ucc_accuracy = batch_correct_predictions / batch_size\n",
    "\n",
    "        # compute the ucc_loss between [batch, 4], [Batch,] batch size = 2\n",
    "        ucc_loss = self.ucc_loss_criterion(ucc_logits, ucc_labels)\n",
    "\n",
    "        # calculate batchwise accuracy/ucc_loss\n",
    "        if is_train_mode:\n",
    "            self.train_ucc_correct_predictions += batch_correct_predictions\n",
    "            self.train_ucc_total_batches += batch_size\n",
    "        else:\n",
    "            self.eval_ucc_correct_predictions += batch_correct_predictions\n",
    "            self.eval_ucc_total_batches += batch_size\n",
    "        return ucc_loss, batch_ucc_accuracy\n",
    "\n",
    "    '''\n",
    "    NOTE: To improve this I can also add a rcc-ucc-enforcement loss where the number of unique classes should match the ucc exactly\n",
    "    '''\n",
    "    def calculate_rcc_loss_and_acc(self, rcc_logits, rcc_labels, is_train_mode=True):\n",
    "        # compute the rcc_loss between [batch, 10] ( as there are 10 classes)\n",
    "        batch_times_bag_size = config.batch_size * config.bag_size\n",
    "\n",
    "        # round it to the nearest integer\n",
    "        predicted = torch.round(rcc_logits).to(torch.float32)\n",
    "\n",
    "        # NOTE: not sure if it is dim\n",
    "        batch_correct_predictions = (predicted == rcc_labels).sum().item()\n",
    "\n",
    "        # compute the rcc_loss\n",
    "        rcc_loss = self.rcc_loss_criterion(rcc_logits, rcc_labels)\n",
    "\n",
    "        # calculate batchwise accuracy/ucc_loss\n",
    "        batch_rcc_accuracy = batch_correct_predictions / batch_times_bag_size\n",
    "        if is_train_mode:\n",
    "            self.train_rcc_correct_predictions += batch_correct_predictions\n",
    "            self.train_rcc_total_batches += batch_times_bag_size\n",
    "        else:\n",
    "            self.eval_rcc_correct_predictions += batch_correct_predictions\n",
    "            self.eval_rcc_total_batches += batch_times_bag_size\n",
    "        return rcc_loss, batch_rcc_accuracy\n",
    "\n",
    "    def calculate_avg_train_stats_hook(self):\n",
    "        avg_training_loss_for_epoch = np.mean(np.array(self.debug_total_losses))\n",
    "        avg_ae_loss_for_epoch = np.mean(np.array(self.debug_ae_losses))\n",
    "        avg_ucc_loss_for_epoch = np.mean(np.array(self.debug_ucc_losses))\n",
    "        avg_rcc_loss_for_epoch = np.mean(np.array(self.debug_rcc_losses))\n",
    "        avg_ucc_training_accuracy = self.train_ucc_correct_predictions / self.train_ucc_total_batches\n",
    "        avg_rcc_training_accuracy = self.train_rcc_correct_predictions / self.train_rcc_total_batches\n",
    "\n",
    "        epoch_train_stats = {\n",
    "            \"avg_training_loss\": avg_training_loss_for_epoch,\n",
    "            \"avg_ae_loss\": avg_ae_loss_for_epoch,\n",
    "            \"avg_ucc_loss\": avg_ucc_loss_for_epoch,\n",
    "            \"avg_rcc_loss\": avg_rcc_loss_for_epoch,\n",
    "            \"avg_ucc_training_accuracy\": avg_ucc_training_accuracy,\n",
    "            \"avg_rcc_training_accuracy\": avg_rcc_training_accuracy\n",
    "        }\n",
    "\n",
    "        # reset\n",
    "        self.train_ucc_correct_predictions = 0\n",
    "        self.train_ucc_total_batches = 0\n",
    "\n",
    "        self.train_rcc_correct_predictions = 0\n",
    "        self.train_rcc_total_batches = 0\n",
    "\n",
    "        return epoch_train_stats\n",
    "\n",
    "    def validation_hook(self):\n",
    "        # class level init\n",
    "        self.eval_ucc_correct_predictions = 0\n",
    "        self.eval_ucc_total_batches = 0\n",
    "\n",
    "        self.eval_rcc_correct_predictions = 0\n",
    "        self.eval_rcc_total_batches = 0\n",
    "\n",
    "        val_loss = []\n",
    "        val_ae_loss = []\n",
    "        val_ucc_loss = []\n",
    "        val_rcc_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set all models to eval mode\n",
    "            self.rcc_model.eval()\n",
    "\n",
    "            for val_batch_idx, val_data in enumerate(self.val_loader):\n",
    "                val_images, val_ucc_labels, val_rcc_labels = val_data\n",
    "\n",
    "                # forward propogate through the model\n",
    "                val_rcc_logits, val_ucc_logits, val_decoded = self.rcc_model(val_images)\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                val_batch_ae_loss = self.calculate_autoencoder_loss(val_images, val_decoded)\n",
    "                val_batch_ucc_loss, val_batch_ucc_accuracy = self.calculate_ucc_loss_and_acc(val_ucc_logits,\n",
    "                                                                                             val_ucc_labels,\n",
    "                                                                                             False)\n",
    "                val_batch_rcc_loss, val_batch_rcc_accuracy = self.calculate_rcc_loss_and_acc(val_rcc_logits,\n",
    "                                                                                             val_rcc_labels,\n",
    "                                                                                             False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                val_batch_loss = (self.ae_importance * val_batch_ae_loss) + (self.ucc_importance * val_batch_ucc_loss) + (self.rcc_importance * val_batch_rcc_loss)\n",
    "\n",
    "                # cummulate the losses\n",
    "                val_loss.append(val_batch_loss.item())\n",
    "                val_ae_loss.append(val_batch_ae_loss.item())\n",
    "                val_ucc_loss.append(val_batch_ucc_loss.item())\n",
    "                val_rcc_loss.append(val_batch_rcc_loss.item())\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_val_loss = np.mean(np.array(val_loss))\n",
    "        avg_val_ucc_loss = np.mean(np.array(val_ucc_loss))\n",
    "        avg_val_ae_loss = np.mean(np.array(val_ae_loss))\n",
    "        avg_val_rcc_loss = np.mean(np.array(val_rcc_loss))\n",
    "        avg_val_ucc_training_accuracy = self.eval_ucc_correct_predictions / self.eval_ucc_total_batches\n",
    "        avg_val_rcc_training_accuracy = self.eval_rcc_correct_predictions / self.eval_rcc_total_batches\n",
    "\n",
    "        stats = {\n",
    "            \"avg_val_loss\": avg_val_loss,\n",
    "            \"avg_val_ae_loss\": avg_val_ae_loss,\n",
    "            \"avg_val_ucc_loss\": avg_val_ucc_loss,\n",
    "            \"avg_val_rcc_loss\": avg_val_rcc_loss,\n",
    "            \"avg_val_ucc_training_accuracy\": avg_val_ucc_training_accuracy,\n",
    "            \"avg_val_rcc_training_accuracy\": avg_val_rcc_training_accuracy\n",
    "        }\n",
    "\n",
    "        print(stats)\n",
    "        print(\"Finished computing val stats, now showing a sample reconstruction\")\n",
    "\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.val_loader)\n",
    "        return stats\n",
    "\n",
    "    def print_stats_and_store_running_history_hook(self, curr_step, avg_train_stats, avg_val_stats):\n",
    "        loss = avg_train_stats[\"avg_training_loss\"]\n",
    "        ae_loss = avg_train_stats[\"avg_ae_loss\"]\n",
    "        ucc_loss = avg_train_stats[\"avg_ucc_loss\"]\n",
    "        rcc_loss = avg_train_stats[\"avg_rcc_loss\"]\n",
    "        ucc_accuracy = avg_train_stats[\"avg_ucc_training_accuracy\"]\n",
    "        rcc_accuracy = avg_train_stats[\"avg_rcc_training_accuracy\"]\n",
    "\n",
    "        val_loss = avg_val_stats[\"avg_val_loss\"]\n",
    "        val_ae_loss = avg_val_stats[\"avg_val_ae_loss\"]\n",
    "        val_ucc_loss = avg_val_stats[\"avg_val_ucc_loss\"]\n",
    "        val_rcc_loss = avg_val_stats[\"avg_val_rcc_loss\"]\n",
    "        val_ucc_accuracy = avg_val_stats[\"avg_val_ucc_training_accuracy\"]\n",
    "        val_rcc_accuracy = avg_val_stats[\"avg_val_rcc_training_accuracy\"]\n",
    "\n",
    "        # store running history\n",
    "        self.steps.append(curr_step)\n",
    "        self.training_losses.append(loss)\n",
    "        self.training_ae_losses.append(ae_loss)\n",
    "        self.training_ucc_losses.append(ucc_loss)\n",
    "        self.training_rcc_losses.append(rcc_loss)\n",
    "        self.training_ucc_accuracies.append(ucc_accuracy)\n",
    "        self.training_rcc_accuracies.append(rcc_accuracy)\n",
    "\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_ae_losses.append(val_ae_loss)\n",
    "        self.val_ucc_losses.append(val_ucc_loss)\n",
    "        self.val_rcc_losses.append(val_rcc_loss)\n",
    "        self.val_ucc_accuracies.append(val_ucc_accuracy)\n",
    "        self.val_rcc_accuracies.append(val_rcc_accuracy)\n",
    "\n",
    "        # print stats\n",
    "        print(\n",
    "            f\"[TRAIN]:Step: {curr_step} | Loss: {loss} | AE Loss: {ae_loss} | UCC Loss: {ucc_loss} | UCC Acc: {ucc_accuracy} | RCC Loss: {rcc_loss} | RCC Acc: {rcc_accuracy}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"[VAL]:Step: {curr_step} | Val Loss: {val_loss} | Val AE Loss: {val_ae_loss} | Val UCC Loss: {val_ucc_loss} | Val UCC Acc: {val_ucc_accuracy} | Val RCC Loss: {val_rcc_loss} | Val RCC Acc: {val_rcc_accuracy}\"\n",
    "        )\n",
    "        print()\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    def get_current_running_history_state_hook(self):\n",
    "        return self.steps, \\\n",
    "            self.training_ae_losses, self.training_ucc_losses, self.training_rcc_losses, self.training_losses, self.training_ucc_accuracies, self.training_rcc_accuracies, \\\n",
    "            self.val_ae_losses, self.val_ucc_losses, self.val_rcc_losses, self.val_losses, self.val_ucc_accuracies, self.val_rcc_accuracies\n",
    "\n",
    "    def save_model_checkpoint_hook(self, step):\n",
    "        # set it to train mode to save the weights (but doesn't matter apparently!)\n",
    "        self.rcc_model.train()\n",
    "\n",
    "        # create the directory if it doesn't exist\n",
    "        model_save_directory = os.path.join(self.save_dir, self.name)\n",
    "        os.makedirs(model_save_directory, exist_ok=True)\n",
    "\n",
    "        # Checkpoint the model at the end of each epoch\n",
    "        checkpoint_path = os.path.join(model_save_directory, f'model_epoch_{step + 1}.pt')\n",
    "        torch.save(\n",
    "            {\n",
    "                'rcc_model_state_dict': self.rcc_model.state_dict(),\n",
    "                'rcc_optimizer_state_dict': self.rcc_optimizer.state_dict(),\n",
    "                'steps': self.steps,\n",
    "                'training_losses': self.training_losses,\n",
    "                'training_ae_losses': self.training_ae_losses,\n",
    "                'training_ucc_losses': self.training_ucc_losses,\n",
    "                'training_rcc_losses': self.training_rcc_losses,\n",
    "                'training_ucc_accuracies': self.training_ucc_accuracies,\n",
    "                'training_rcc_accuracies': self.training_rcc_accuracies,\n",
    "                'val_losses': self.val_losses,\n",
    "                'val_ae_losses': self.val_ae_losses,\n",
    "                'val_ucc_losses': self.val_ucc_losses,\n",
    "                'val_rcc_losses': self.val_rcc_losses,\n",
    "                'val_ucc_accuracies': self.val_ucc_accuracies,\n",
    "                'val_rcc_accuracies': self.val_rcc_accuracies\n",
    "            },\n",
    "            checkpoint_path\n",
    "        )\n",
    "        print(f\"Saved the model checkpoint for experiment {self.name} for epoch {step + 1}\")\n",
    "\n",
    "    def test_model(self):\n",
    "        # class level init\n",
    "        self.eval_ucc_correct_predictions = 0\n",
    "        self.eval_ucc_total_batches = 0\n",
    "\n",
    "        self.eval_rcc_correct_predictions = 0\n",
    "        self.eval_rcc_total_batches = 0\n",
    "\n",
    "        test_loss = []\n",
    "        test_ae_loss = []\n",
    "        test_ucc_loss = []\n",
    "        test_rcc_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set all models to eval mode\n",
    "            self.rcc_model.eval()\n",
    "\n",
    "            for test_batch_idx, test_data in enumerate(self.test_loader):\n",
    "                test_images, test_ucc_labels, test_rcc_labels = test_data\n",
    "\n",
    "                # forward propogate through the model\n",
    "                test_rcc_logits, test_ucc_logits, test_decoded = self.rcc_model(test_images)\n",
    "\n",
    "                # calculate losses from both models for a batch of bags\n",
    "                test_batch_ae_loss = self.calculate_autoencoder_loss(test_images, test_decoded)\n",
    "                test_batch_ucc_loss, test_batch_ucc_accuracy = self.calculate_ucc_loss_and_acc(test_ucc_logits,\n",
    "                                                                                               test_ucc_labels,\n",
    "                                                                                               False)\n",
    "                test_batch_rcc_loss, test_batch_rcc_accuracy = self.calculate_rcc_loss_and_acc(test_rcc_logits,\n",
    "                                                                                               test_rcc_labels,\n",
    "                                                                                               False)\n",
    "\n",
    "                # calculate combined loss\n",
    "                test_batch_loss = (self.ae_importance * test_batch_ae_loss) + (self.ucc_importance * test_batch_ucc_loss) + (self.rcc_importance * test_batch_rcc_loss)\n",
    "\n",
    "                # cummulate the losses\n",
    "                test_loss.append(test_batch_loss.item())\n",
    "                test_ae_loss.append(test_batch_ae_loss.item())\n",
    "                test_ucc_loss.append(test_batch_ucc_loss.item())\n",
    "                test_rcc_loss.append(test_batch_rcc_loss.item())\n",
    "\n",
    "        # Calculate average validation loss for the epoch\n",
    "        avg_test_loss = np.mean(np.array(test_loss))\n",
    "        avg_test_ucc_loss = np.mean(np.array(test_ucc_loss))\n",
    "        avg_test_rcc_loss = np.mean(np.array(test_rcc_loss))\n",
    "        avg_test_ae_loss = np.mean(np.array(test_ae_loss))\n",
    "        avg_test_ucc_training_accuracy = self.eval_ucc_correct_predictions / self.eval_ucc_total_batches\n",
    "        avg_test_rcc_training_accuracy = self.eval_rcc_correct_predictions / self.eval_rcc_total_batches\n",
    "\n",
    "        stats = {\n",
    "            \"avg_test_loss\": avg_test_loss,\n",
    "            \"avg_test_ae_loss\": avg_test_ae_loss,\n",
    "            \"avg_test_ucc_loss\": avg_test_ucc_loss,\n",
    "            \"avg_test_rcc_loss\": avg_test_rcc_loss,\n",
    "            \"avg_test_ucc_training_accuracy\": avg_test_ucc_training_accuracy,\n",
    "            \"avg_test_rcc_training_accuracy\": avg_test_rcc_training_accuracy\n",
    "        }\n",
    "        print(stats)\n",
    "\n",
    "        print(\"Now going to show a sample reconstruction\")\n",
    "        # show some sample predictions\n",
    "        self.show_sample_reconstructions(self.test_loader)\n",
    "        return stats\n",
    "\n",
    "    def show_sample_reconstructions(self, dataloader):\n",
    "        # Create a subplot grid\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(3, 3))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # set all models to eval mode\n",
    "            self.rcc_model.eval()\n",
    "\n",
    "            for val_data in dataloader:\n",
    "                val_images, _, _ = val_data\n",
    "\n",
    "                # reshape to appropriate size\n",
    "                batch_size, bag_size, num_channels, height, width = val_images.size()\n",
    "                bag_val_images = val_images.view(batch_size * bag_size, num_channels, height, width)\n",
    "                print(\"Reshaped the original image into bag format\")\n",
    "\n",
    "                # forward propagate through the model\n",
    "                _, _, val_reconstructed_images = self.rcc_model(val_images)\n",
    "                print(\"Got a sample reconstruction, now trying to reshape in order to show an example\")\n",
    "\n",
    "                # take only one image from the bag\n",
    "                sample_image = bag_val_images[0]\n",
    "                predicted_image = val_reconstructed_images[0]\n",
    "\n",
    "                # get it to cpu\n",
    "                sample_image = sample_image.to(\"cpu\")\n",
    "                predicted_image = predicted_image.to(\"cpu\")\n",
    "\n",
    "                # convert to PIL Image\n",
    "                sample_image = self.tensor_to_img_transform(sample_image)\n",
    "                predicted_image = self.tensor_to_img_transform(predicted_image)\n",
    "\n",
    "                axes[0].imshow(sample_image)\n",
    "                axes[0].set_title(f\"Orig\", color='green')\n",
    "                axes[0].axis('off')\n",
    "\n",
    "                axes[1].imshow(predicted_image)\n",
    "                axes[1].set_title(f\"Recon\", color='red')\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                # show only one image\n",
    "                break\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # find the most recent file and return the path\n",
    "    def get_model_checkpoint_path(self, step_num=None):\n",
    "        directory = os.path.join(self.save_dir, self.name)\n",
    "        if step_num == None:\n",
    "            # Get a list of all files in the directory\n",
    "            files = os.listdir(directory)\n",
    "\n",
    "            # Filter out only the files (exclude directories)\n",
    "            files = [f for f in files if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "            # Sort the files by their modification time in descending order (most recent first)\n",
    "            files.sort(key=lambda x: os.path.getmtime(os.path.join(directory, x)), reverse=True)\n",
    "\n",
    "            # Get the name of the most recently added file\n",
    "            model_file = files[0] if files else None\n",
    "        else:\n",
    "            model_file = f\"model_step_{step_num}.pt\"\n",
    "        return os.path.join(directory, model_file)\n",
    "\n",
    "    # Calculate min JS Divergence\n",
    "    def calculate_js_divergence(self, p, q):\n",
    "        m = 0.5 * (p + q)\n",
    "        log_p_over_m = np.log2(p / m)\n",
    "        log_q_over_m = np.log2(q / m)\n",
    "        return 0.5 * np.sum(p * log_p_over_m) + 0.5 * np.sum(q * log_q_over_m)\n",
    "\n",
    "    def get_all_kde_distributions(self):\n",
    "        self.rcc_model.eval()\n",
    "\n",
    "        kde_distributions = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.cifar_test_loader):\n",
    "                imgs = imgs.unsqueeze(1).to(config.device)\n",
    "                kde_dist = self.rcc_model.get_kde_distributions(imgs)\n",
    "                kde_distributions.append(kde_dist.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        kde_distributions = np.concatenate(kde_distributions)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        return kde_distributions, all_labels\n",
    "\n",
    "    def compute_min_js_divergence(self):\n",
    "        kde_distributions, all_labels = self.get_all_kde_distributions()\n",
    "        print(\"Got the KDE distributions for the test dataset\")\n",
    "\n",
    "        # iterate for each class and get only those embeddings\n",
    "        distribution_of_all_label_classes = []\n",
    "        for i in range(config.num_classes):\n",
    "            idxs = np.where(all_labels == i)\n",
    "            kde_distribution_i = kde_distributions[idxs]\n",
    "            kde_distribution_i = np.mean(kde_distribution_i, axis=0)\n",
    "            distribution_of_all_label_classes.append(kde_distribution_i)\n",
    "        distribution_of_all_label_classes = np.array(distribution_of_all_label_classes)\n",
    "        print(\"Got the average kde distribution per label class, now computing min js divergence\")\n",
    "\n",
    "        res = np.zeros((config.num_classes, config.num_classes))\n",
    "        for i in range(config.num_classes):\n",
    "            p = np.clip(distribution_of_all_label_classes[i, :], 1e-12, 1)\n",
    "            # p = distribution_of_all_label_classes[i, :]\n",
    "            for j in range(i, config.num_classes):\n",
    "                q = np.clip(distribution_of_all_label_classes[j, :], 1e-12, 1)\n",
    "                # q = distribution_of_all_label_classes[j, :]\n",
    "\n",
    "                # fill the upper triangle\n",
    "                res[i, j] = self.calculate_js_divergence(p, q)\n",
    "\n",
    "                # fill the lower triangle\n",
    "                res[j, i] = res[i, j]\n",
    "\n",
    "        # we are not interested in the identity relation anyway\n",
    "        np.fill_diagonal(res, np.inf)\n",
    "        print(\"Computed all interclass js divergence scores, the entire interclass js divergence is \")\n",
    "        print(res)\n",
    "        # Find the minimum value\n",
    "        min_js_divergence = np.min(res)\n",
    "\n",
    "        # Find the indices of the minimum value\n",
    "        min_indices = np.argmin(res)\n",
    "        min_row, min_col = np.unravel_index(min_indices, res.shape)\n",
    "\n",
    "        print(f\"Min JS Divergence is {min_js_divergence}, between classes {min_row} & {min_col}\")\n",
    "        return min_js_divergence\n",
    "\n",
    "\n",
    "    # Calculate clustering accuracy\n",
    "    def get_all_encoder_features(self):\n",
    "        self.rcc_model.eval()\n",
    "\n",
    "        all_features = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in tqdm(self.cifar_test_loader):\n",
    "                imgs = imgs.unsqueeze(1).to(config.device)\n",
    "                features = self.rcc_model.get_encoder_features(imgs)\n",
    "                all_features.append(features.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_features = np.concatenate(all_features)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        return all_features, all_labels\n",
    "\n",
    "    def perform_clustering_and_get_cluster_labels(self, features):\n",
    "        model = KMeans(n_clusters=10, init='k-means++', n_init=10)\n",
    "        model.fit(features)\n",
    "        return model.labels_\n",
    "\n",
    "    def compute_clustering_accuracy(self):\n",
    "        all_features, all_labels = self.get_all_encoder_features()\n",
    "        print(\"Computed all the features from the encoder\")\n",
    "        cluster_labels = self.perform_clustering_and_get_cluster_labels(all_features)\n",
    "        print(\"Performed clustering and computed all the cluster labels\")\n",
    "\n",
    "        cost_matrix = np.zeros((config.num_classes, config.num_classes))\n",
    "        num_samples = np.zeros(config.num_classes)\n",
    "\n",
    "        for true_label in range(config.num_classes):\n",
    "            true_label_idxs = np.where(all_labels == true_label)[0]\n",
    "            num_samples[true_label] = true_label_idxs.shape[0]\n",
    "\n",
    "            sample_preds = cluster_labels[true_label_idxs]\n",
    "\n",
    "            for pred_label in range(config.num_classes):\n",
    "                pairs = np.where(sample_preds == pred_label)[0]\n",
    "\n",
    "                cost_matrix[true_label, pred_label] = 1 - (pairs.shape[0] / true_label_idxs.shape[0])\n",
    "\n",
    "        print(\"Going to perform linear sum assignment of cost matrix\")\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        cost = cost_matrix[row_ind, col_ind]\n",
    "        clustering_acc = ((1 - cost) * num_samples).sum() / num_samples.sum()\n",
    "        print(f\"Clustering accuracy is {clustering_acc}\")\n",
    "        return clustering_acc\n"
   ],
   "metadata": {
    "id": "K372cEh9ht_Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the model instances\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tahh6ZQHht_Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment2 = \"rcc\"\n",
    "save_dir = os.path.abspath(config.weights_path)\n",
    "rcc_model = RCCModel(device=config.device).to(config.device)\n",
    "\n",
    "#creating the trainer\n",
    "rcc_trainer = RCCTrainer(experiment2, rcc_model, dataloaders, save_dir)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3Ubg9WCht_Q",
    "outputId": "311a5e12-06fa-42cb-e53c-76aae9624595"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GpvMO6SOht_R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Going to start training..\")\n",
    "exp2_steps, exp2_training_ae_losses, exp2_training_ucc_losses, exp2_training_rcc_losses, exp2_training_losses, exp2_training_ucc_accuracies, exp2_training_rcc_accuracies, exp2_val_ae_losses, exp2_val_ucc_losses, exp2_val_rcc_losses, exp2_val_losses, exp2_val_ucc_accuracies, exp2_val_rcc_accuracies = rcc_trainer.train()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "558de28cbe8c4708bc987bc61d755afc",
      "e4847c8ca55b45d18671e8f77be09edd",
      "a2fcc97e6a1748f2912c4ae3905fa96f",
      "1678d24dfa8144ba9552be295229b213",
      "83a4e28c325d4878a9247a04beb05df6",
      "01c6e0633de241818c22e1c37eb9a13e",
      "9ba458799ff34aefac058e2622c704a8",
      "d05cdd13514d42f98cb05db79c2a0540",
      "8007f8ec4599411b9fd85efd92811ffb",
      "e83f228ec0334011b556470efb27e3fb",
      "2320a01b03fa460687e9fbd95d25c360"
     ]
    },
    "id": "RnrniNW_ht_T",
    "outputId": "5bdd0454-56b3-46f2-f117-92871d47c8e2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Additional Training if required"
   ],
   "metadata": {
    "collapsed": false,
    "id": "P8d7PMEdht_U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# exp2_steps, exp2_training_ae_losses, exp2_training_ucc_losses, exp2_training_rcc_losses, exp2_training_losses, exp2_training_ucc_accuracies, exp2_training_rcc_accuracies, exp2_val_ae_losses, exp2_val_ucc_losses, exp2_val_rcc_losses, exp2_val_losses, exp2_val_ucc_accuracies, exp2_val_rcc_accuracies = rcc_trainer.train(load_from_checkpoint=True, resume_steps=Some number)"
   ],
   "metadata": {
    "is_executing": true,
    "id": "hxCh-ExNht_U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plotting the model stats"
   ],
   "metadata": {
    "collapsed": false,
    "id": "WwoZVUHDht_U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_ucc_rcc_model_stats(experiment2, exp2_steps, exp2_training_ucc_losses, exp2_training_ae_losses, exp2_training_rcc_losses,exp2_training_losses, exp2_training_ucc_accuracies, exp2_training_rcc_accuracies, exp2_val_ucc_losses, exp2_val_ae_losses, exp2_val_rcc_losses, exp2_val_losses, exp2_val_ucc_accuracies, exp2_val_rcc_accuracies)"
   ],
   "metadata": {
    "is_executing": true,
    "id": "neEvotGsht_U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing the model"
   ],
   "metadata": {
    "collapsed": false,
    "id": "D4GhKjJKht_U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rcc_trainer.test_model()"
   ],
   "metadata": {
    "is_executing": true,
    "id": "zl10Kxt2ht_U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating the Min JS Divergence"
   ],
   "metadata": {
    "collapsed": false,
    "id": "GZnH4eRYht_U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp1_min_js_divg = rcc_trainer.compute_min_js_divergence()\n",
    "exp1_min_js_divg"
   ],
   "metadata": {
    "is_executing": true,
    "id": "jKnPTx1Sht_U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculating the Clustering Accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "id": "lfEASpnkht_U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp1_clustering_accuracies = rcc_trainer.compute_clustering_accuracy()\n",
    "exp1_clustering_accuracies"
   ],
   "metadata": {
    "is_executing": true,
    "id": "dBTbeo4Sht_U"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "558de28cbe8c4708bc987bc61d755afc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e4847c8ca55b45d18671e8f77be09edd",
       "IPY_MODEL_a2fcc97e6a1748f2912c4ae3905fa96f",
       "IPY_MODEL_1678d24dfa8144ba9552be295229b213"
      ],
      "layout": "IPY_MODEL_83a4e28c325d4878a9247a04beb05df6"
     }
    },
    "e4847c8ca55b45d18671e8f77be09edd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01c6e0633de241818c22e1c37eb9a13e",
      "placeholder": "​",
      "style": "IPY_MODEL_9ba458799ff34aefac058e2622c704a8",
      "value": "Steps:  52%"
     }
    },
    "a2fcc97e6a1748f2912c4ae3905fa96f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d05cdd13514d42f98cb05db79c2a0540",
      "max": 60000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8007f8ec4599411b9fd85efd92811ffb",
      "value": 32000
     }
    },
    "1678d24dfa8144ba9552be295229b213": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e83f228ec0334011b556470efb27e3fb",
      "placeholder": "​",
      "style": "IPY_MODEL_2320a01b03fa460687e9fbd95d25c360",
      "value": " 32000/60000 [3:24:38&lt;3:19:50,  2.34it/s, batch_loss=0.697, batch_ae_loss=0.00798, batch_ucc_loss=1.39, batch_ucc_acc=0.3]"
     }
    },
    "83a4e28c325d4878a9247a04beb05df6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "01c6e0633de241818c22e1c37eb9a13e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ba458799ff34aefac058e2622c704a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d05cdd13514d42f98cb05db79c2a0540": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8007f8ec4599411b9fd85efd92811ffb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": "green",
      "description_width": ""
     }
    },
    "e83f228ec0334011b556470efb27e3fb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2320a01b03fa460687e9fbd95d25c360": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
